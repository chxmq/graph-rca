# Graph-RCA: Log Analysis & Incident Resolution System

An AI-powered system for automated log analysis and incident resolution. Features a modern React frontend with a FastAPI backend, leveraging LLMs for root cause analysis and solution generation.

## ğŸ¯ Features

- ğŸ“Š **Automated log file analysis** with causal chain generation
- ğŸ” **Root cause identification** using graph-based analysis
- ğŸ“ **Documentation integration** for context-aware solutions
- ğŸ¤– **AI-powered incident resolution** using RAG (Retrieval Augmented Generation)
- ğŸ’ **Modern glass-morphism UI** built with React + TypeScript + Tailwind
- ğŸš€ **FastAPI backend** with RESTful endpoints
- ğŸ—„ï¸ **Persistent storage** with MongoDB and ChromaDB

## ğŸ“ Project Structure

```
graph-rca/
â”œâ”€â”€ backend/                 # Python FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/            # API routes
â”‚   â”‚   â”œâ”€â”€ core/           # Database handlers, embeddings, RAG engine
â”‚   â”‚   â”œâ”€â”€ models/         # Pydantic data models
â”‚   â”‚   â””â”€â”€ utils/          # Log parser, graph generator, context builder
â”‚   â”œâ”€â”€ tests/              # Backend tests
â”‚   â”œâ”€â”€ main.py             # FastAPI application entry point
â”‚   â””â”€â”€ requirements.txt    # Python dependencies
â”‚
â”œâ”€â”€ frontend/               # React TypeScript frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/    # React components
â”‚   â”‚   â”œâ”€â”€ api.ts         # API client
â”‚   â”‚   â”œâ”€â”€ store.ts       # State management (Zustand)
â”‚   â”‚   â””â”€â”€ App.tsx        # Main application
â”‚   â””â”€â”€ package.json       # Node dependencies
â”‚
â”œâ”€â”€ docs/                  # Documentation and samples
â”œâ”€â”€ data/                  # Docker volume mounts (auto-created)
â”œâ”€â”€ docker-compose.yaml    # Docker services (MongoDB, ChromaDB, Ollama)
â””â”€â”€ run.sh                 # Master setup script (run this first!)
```

## ğŸš€ Quick Start

### Prerequisites

- **Docker & Docker Compose**
- **Python 3.13+**
- **Node.js 18+**
- **NVIDIA GPU** (optional, for faster LLM inference - Linux only)

### One-Command Setup

```bash
git clone https://github.com/KTS-o7/graph-rca.git
cd graph-rca
./run.sh
```

That's it! The script will:
- âœ“ Check Docker
- âœ“ Start all Docker services (MongoDB, ChromaDB, Ollama)
- âœ“ Download the LLM model if needed
- âœ“ Set up Python virtual environment
- âœ“ Install all Python dependencies
- âœ“ Install all Node.js dependencies
- âœ“ Create convenience start scripts

### Start the Application

After running `./run.sh`, open **two separate terminals**:

**Terminal 1 - Backend:**
```bash
./start-backend.sh
```

**Terminal 2 - Frontend:**
```bash
./start-frontend.sh
```

Then open your browser to `http://localhost:5173`

### What Gets Started

| Service | URL | Description |
|---------|-----|-------------|
| **Frontend** | http://localhost:5173 | React UI |
| **Backend API** | http://localhost:8010 | FastAPI server |
| **API Docs** | http://localhost:8010/docs | Interactive API documentation |
| MongoDB | localhost:27017 | Database |
| ChromaDB | localhost:8000 | Vector database |
| Ollama | localhost:11435 | LLM inference |

## ğŸ“– Usage

### Step-by-step workflow:

1. **Analyse Log File**
   - Navigate to the "1. Log Analysis" tab
   - Upload a `.log` or `.txt` file
   - Click "Analyse log"
   - View severity, root cause, and summary

2. **Add Documentation** (Optional but recommended)
   - Go to "2. Documentation" tab
   - Upload runbooks, guides, or service docs (`.txt` or `.md`)
   - Click "Index documentation"

3. **Generate Resolution**
   - Open "3. Incident Resolution" tab
   - Click "Run resolution"
   - Review the generated solution with references

## ğŸ”§ API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/` | API information |
| `GET` | `/api/health` | Health check |
| `POST` | `/api/log/analyse` | Analyse uploaded log file |
| `POST` | `/api/docs/upload` | Upload documentation |
| `POST` | `/api/incident/resolve` | Generate incident resolution |

Full API docs: `http://localhost:8010/docs`

## ğŸ³ Docker Services

Check running services:
```bash
docker-compose ps
```

View logs:
```bash
docker-compose logs <service-name>
# Examples:
docker-compose logs ollama
docker-compose logs chroma
docker-compose logs mongodb
```

Stop all services:
```bash
docker-compose down
```

## ğŸ› ï¸ Development

### Backend Development

```bash
cd backend
source venv/bin/activate
python -m uvicorn main:app --reload --host 0.0.0.0 --port 8010
```

### Frontend Development

```bash
cd frontend
npm run dev
```

### Run Tests

```bash
cd backend
source venv/bin/activate
pytest tests/
```

### Stop All Services

```bash
docker-compose down
# Kill backend/frontend with Ctrl+C in their terminals
```

## ğŸ¨ Tech Stack

**Frontend:**
- React 18 + TypeScript
- Vite (build tool)
- Tailwind CSS (styling)
- Zustand (state management)
- React Icons

**Backend:**
- FastAPI (Python web framework)
- Pydantic (data validation)
- Ollama (local LLM inference)
- LangChain (text processing)

**Databases:**
- ChromaDB (vector embeddings)
- MongoDB (structured data)

## ğŸ” Troubleshooting

**Backend won't start:**
- Ensure Docker services are running: `docker ps`
- Check Ollama has the model: `docker exec -it $(docker ps -qf "name=ollama") ollama list`

**Frontend can't connect:**
- Verify backend is running on port 8010
- Check CORS settings in `backend/main.py`

**GPU Support (Optional):**
- By default, Ollama runs on CPU (works on all systems)
- For NVIDIA GPU acceleration (Linux only):
  ```bash
  docker-compose -f docker-compose.yaml -f docker-compose.gpu.yaml up -d
  ```
- Requires: NVIDIA drivers, CUDA, and nvidia-docker2 installed

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.
