{
  "total_incidents": 200,
  "total_runs": 600,
  "overall_accuracy": 0.7733,
  "avg_score_quality": 0.6675,
  "overall_std": 0.3384,
  "by_category": {
    "Configuration": {
      "correct": 13,
      "total": 18,
      "scores": [
        0.7,
        0.4,
        0.4,
        0.7,
        0.7,
        0.7,
        0.9,
        1.0,
        0.8,
        0,
        0,
        0,
        0.7,
        0.7,
        0.7,
        0.7,
        0.7,
        0.7
      ],
      "accuracy": 0.722,
      "avg_score": 0.583
    },
    "Network": {
      "correct": 57,
      "total": 72,
      "scores": [
        0.0,
        1.0,
        0.0,
        0.0,
        1.0,
        0.0,
        0.9,
        0.7,
        0.7,
        0.7,
        0.7,
        0.7,
        0.7,
        0.8,
        0.7,
        1.0,
        1.0,
        1.0,
        0.8,
        0.9,
        0.7,
        1.0,
        0.7,
        0.9,
        1.0,
        1.0,
        0.9,
        1.0,
        1.0,
        1.0,
        0,
        0,
        0,
        0.8,
        0.7,
        0.7,
        0.7,
        0.7,
        0.4,
        0.5,
        0.7,
        0.7,
        1.0,
        1.0,
        1.0,
        0.0,
        0.3,
        0.0,
        0.7,
        1.0,
        0.7,
        0.9,
        1.0,
        1.0,
        0.7,
        0.7,
        0.7,
        0.8,
        0.7,
        0.8,
        0.8,
        0.8,
        0.7,
        0.7,
        0.7,
        0.7,
        1.0,
        0.7,
        1.0,
        0.0,
        0.1,
        0.0
      ],
      "accuracy": 0.792,
      "avg_score": 0.675
    },
    "CI-CD": {
      "correct": 0,
      "total": 3,
      "scores": [
        0.0,
        0.0,
        0.0
      ],
      "accuracy": 0.0,
      "avg_score": 0.0
    },
    "Database": {
      "correct": 101,
      "total": 138,
      "scores": [
        1.0,
        1.0,
        1.0,
        1.0,
        0.8,
        0.7,
        0.8,
        0.8,
        0.8,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        0.8,
        0.7,
        0.7,
        0.7,
        0.7,
        0.7,
        0.4,
        0.4,
        0.4,
        0.7,
        0.7,
        0.7,
        0.8,
        0.8,
        0.7,
        0.7,
        0.8,
        0.7,
        0.8,
        0.7,
        0.7,
        0.7,
        0.7,
        0.8,
        0.7,
        0.8,
        0.8,
        0.7,
        0.7,
        0.7,
        0.0,
        0.0,
        0.0,
        0.8,
        0.7,
        0.8,
        0.7,
        0.8,
        0.7,
        0.7,
        0.7,
        0.7,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.8,
        0.9,
        0.8,
        1.0,
        1.0,
        1.0,
        0.0,
        1.0,
        0.0,
        0.9,
        0.7,
        0.7,
        0.1,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        0.0,
        0.0,
        0.0,
        0.7,
        0.4,
        0.8,
        0.7,
        0.7,
        0.8,
        0.7,
        1.0,
        1.0,
        0.7,
        0.7,
        0.8,
        0.4,
        0.4,
        0.4,
        0.4,
        0.7,
        0.7,
        1.0,
        1.0,
        1.0,
        0.7,
        0.6,
        0.8,
        0.0,
        0.0,
        0.0,
        0.8,
        1.0,
        1.0,
        0.7,
        0.9,
        0.7,
        1.0,
        0.7,
        0.7,
        0.0,
        0.0,
        0.0,
        0.7,
        0.7,
        0.7,
        0.4,
        0.4,
        0.6,
        0.0,
        0.0,
        0.4,
        0.4,
        0.7,
        0.7,
        0.7,
        1.0,
        0.8
      ],
      "accuracy": 0.732,
      "avg_score": 0.638
    },
    "Software": {
      "correct": 68,
      "total": 93,
      "scores": [
        0.0,
        0.7,
        0.0,
        0.0,
        0.0,
        0.0,
        0.7,
        0.7,
        0.7,
        1.0,
        1.0,
        1.0,
        0,
        0.7,
        0.7,
        0.4,
        0.4,
        0.7,
        1.0,
        1.0,
        1.0,
        0.7,
        0.7,
        0.7,
        1.0,
        1.0,
        1.0,
        0.7,
        0.7,
        0.7,
        0.7,
        0.7,
        0.7,
        0.4,
        0.4,
        1.0,
        0.8,
        0.9,
        0.8,
        0.7,
        0.7,
        0.7,
        0.7,
        0.4,
        0.4,
        0.8,
        0.7,
        0.7,
        0.0,
        0,
        0,
        0.7,
        0.7,
        0.7,
        0.0,
        0.0,
        0.0,
        0.7,
        0.8,
        0.7,
        0.9,
        0.8,
        0.8,
        0.4,
        0.9,
        1.0,
        0.7,
        0.7,
        0.7,
        0.8,
        1.0,
        0.7,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.7,
        0.7,
        0.7,
        0.8,
        0.8,
        0.7,
        0.9,
        0.9,
        0.9
      ],
      "accuracy": 0.731,
      "avg_score": 0.623
    },
    "Infrastructure": {
      "correct": 171,
      "total": 198,
      "scores": [
        0.8,
        0.7,
        0.7,
        0.8,
        0.7,
        0.7,
        1.0,
        1.0,
        1.0,
        0.9,
        0.7,
        0.7,
        0.8,
        1.0,
        1.0,
        0.9,
        0.9,
        0.9,
        1.0,
        1.0,
        1.0,
        0.7,
        0.7,
        0.7,
        0.7,
        0.8,
        0.7,
        0.7,
        1.0,
        0.8,
        0.9,
        0.9,
        0.8,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        0.7,
        0.7,
        0.8,
        1.0,
        1.0,
        1.0,
        0.8,
        0.7,
        0.8,
        1.0,
        0.9,
        0.9,
        0.0,
        0.0,
        0.0,
        0.7,
        0.7,
        0.7,
        0.4,
        0.4,
        0.7,
        0.7,
        0.7,
        0.7,
        0.7,
        0.7,
        0.7,
        1.0,
        1.0,
        0.7,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        0.7,
        0.7,
        0.7,
        0.0,
        0.0,
        0.0,
        0.8,
        1.0,
        0.7,
        0.0,
        0.0,
        0.0,
        1.0,
        1.0,
        1.0,
        0.4,
        0.4,
        0.3,
        0.9,
        0.9,
        0.0,
        1.0,
        0.7,
        0.9,
        0.7,
        0.7,
        0.0,
        0.0,
        0.4,
        0.0,
        0.7,
        0.8,
        0.8,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        1.0,
        1.0,
        0.9,
        1.0,
        1.0,
        1.0,
        0.7,
        0.7,
        0.7,
        0.8,
        0.7,
        0.7,
        1.0,
        1.0,
        0.7,
        0.7,
        0.7,
        0.8,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        0.0,
        1.0,
        1.0,
        1.0,
        0.7,
        0.8,
        0.8,
        0.7,
        0.7,
        0.3,
        1.0,
        0.7,
        0.7,
        0.9,
        0.7,
        0.9,
        0.7,
        0.7,
        0.7,
        0.8,
        0.7,
        0.7,
        1.0,
        0.7,
        1.0,
        1.0,
        1.0,
        1.0,
        0.7,
        0.7,
        0.7,
        0.8,
        0.7,
        1.0,
        0.7,
        1.0,
        0.7,
        1.0,
        1.0,
        1.0,
        0.7,
        0.8,
        0.7,
        0.7,
        0.9,
        0.7,
        0.8,
        0.7,
        0.8,
        0.8,
        0.8,
        0.8,
        0.8,
        0.8,
        0.7
      ],
      "accuracy": 0.864,
      "avg_score": 0.743
    },
    "Security": {
      "correct": 31,
      "total": 51,
      "scores": [
        0.7,
        0.9,
        1.0,
        1.0,
        1.0,
        0.8,
        0.7,
        0.7,
        0.7,
        1.0,
        1.0,
        0.7,
        0.7,
        0,
        0,
        1.0,
        1.0,
        1.0,
        0,
        0,
        0,
        0.0,
        0.0,
        0.0,
        1.0,
        1.0,
        1.0,
        0.7,
        0.7,
        1.0,
        0.0,
        0.0,
        0.0,
        0.7,
        0.9,
        0.7,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.9,
        1.0,
        1.0,
        0.7,
        0.8,
        0.7
      ],
      "accuracy": 0.608,
      "avg_score": 0.524
    },
    "Memory": {
      "correct": 17,
      "total": 18,
      "scores": [
        0.9,
        0.9,
        0.9,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        0.7,
        0.7,
        0.4,
        0.7,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0,
        1.0
      ],
      "accuracy": 0.944,
      "avg_score": 0.9
    },
    "Cloud": {
      "correct": 6,
      "total": 9,
      "scores": [
        1.0,
        0.7,
        0.7,
        1.0,
        1.0,
        1.0,
        0.0,
        0.0,
        0.0
      ],
      "accuracy": 0.667,
      "avg_score": 0.6
    }
  },
  "by_incident": [
    {
      "id": "incident_001",
      "company": "Allegro",
      "category": "Configuration",
      "ground_truth": "Misconfiguration in cluster resource management caused services to reserve excessive resources (CPU/RAM) they didn't use, preventing new instances from starting despite available physical capacity.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is likely a misconfigured or under-resourced database connection pool, resulting in a depletion of available connections, which led to query timeouts and subsequent errors.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is likely a misconfigured or inadequate connection pooling mechanism, which resulted in an insufficient number of available connections being allocated to the database, leading to exhaustion and subsequent timeouts.",
          "score": 0.4
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a misconfigured connection pooling mechanism that has led to an exhaustion of available database connections, resulting in query timeouts and errors.",
          "score": 0.4
        }
      ],
      "avg_score": 0.5,
      "correct": false
    },
    {
      "id": "incident_002",
      "company": "Cloudflare",
      "category": "Network",
      "ground_truth": "A poorly written regular expression in a WAF rule caused CPU exhaustion (catastrophic backtracking) on edge routers.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is likely a misconfigured or inadequate connection pooling mechanism, which resulted in an insufficient number of available connections to handle the incoming queries, leading to a pool exhaustion error.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be a failure in the database connection management system, likely due to an unhandled exception or resource leak that led to the exhaustion of available connections.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is likely a misconfigured or inadequate connection pooling mechanism in the application, resulting in an insufficient number of available connections to handle the incoming traffic.",
          "score": 0.0
        }
      ],
      "avg_score": 0.333,
      "correct": false
    },
    {
      "id": "incident_003",
      "company": "Cloudflare",
      "category": "Network",
      "ground_truth": "Configuration typo in Atlanta router: deactivating a prefix-list instead of the term leaked all BGP routes with high local-preference.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident appears to be a failure in the connection pooling mechanism, which resulted in an exhaustion of available connections and subsequent query timeouts.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is likely a misconfigured or inadequate connection pooling mechanism, which resulted in an insufficient number of available connections to handle the incoming queries, leading to the exhaustion of the pool and subsequent timeouts.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a misconfigured connection pool that resulted in an insufficient number of available connections, leading to a failure to handle incoming queries and subsequent timeouts.",
          "score": 0.0
        }
      ],
      "avg_score": 0.333,
      "correct": false
    },
    {
      "id": "incident_004",
      "company": "TravisCI",
      "category": "CI-CD",
      "ground_truth": "Worker v2.6.2 change in bash exit code handling (login shell) caused false failures; rollback to v2.5.0 failed due to missing Docker Hub tag.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a misconfigured Docker Hub tag that caused a version mismatch between the expected and actual versions of the software, leading to a series of false failures and subsequent rollbacks.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be a misconfigured Docker Hub tag, which caused a series of cascading failures and required multiple attempts at rollback and retrying with an updated tag before resolving the issue.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a misconfigured Docker Hub tag that caused a version mismatch between v2.6.2 and v2.5.0, leading to repeated false failures and subsequent rollbacks until the issue was resolved with an updated tag.",
          "score": 0.0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_005",
      "company": "Railway",
      "category": "Database",
      "ground_truth": "Database index creation on a 1B record table without CONCURRENTLY option locked the table, exhausting all connection slots.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a misconfigured or overloaded PgBouncer, which led to an administrative connection slots overflow and ultimately caused the Postgres service to become unavailable.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is an unexpected surge in high CPU usage on the monitoring server, which led to a chain reaction of events including table lock acquisition by Postgres, connection pool slot exhaustion, and ultimately administrative connection slots overflow due to PgBouncer.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a misconfiguration or inadequate tuning of the PostgreSQL connection pool, leading to an administrative connection slot overflow due to PgBouncer, which resulted in system unavailability.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_006",
      "company": "Azure",
      "category": "Software",
      "ground_truth": "Incorrect date arithmetic (adding 1 year to Feb 29) created certificates with an invalid expiration date (Feb 29, 2013).",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was an invalid certificate expiration date that triggered a cascade of errors and outages across multiple Azure services, ultimately resulting in a global outage.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a misconfiguration or incorrect update in the Azure Certificate Manager that resulted in an invalid expiration date for certificates used by multiple services, leading to a cascade of failures and outages across various Azure services.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was an invalid certificate expiration date that caused a cascade of failures across multiple Azure services, ultimately resulting in a global outage.",
          "score": 0.0
        }
      ],
      "avg_score": 0.233,
      "correct": false
    },
    {
      "id": "incident_007",
      "company": "Joyent",
      "category": "Database",
      "ground_truth": "Locking conflict between PostgreSQL transaction wraparound maintenance (autovacuum) and a global lock query that blocked all operations.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a misconfigured or faulty autovacuum process that has become stuck on acquiring a lock on index 1234, causing a prolonged and blocking lock acquisition that ultimately led to a high-severity locking conflict.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a misconfigured or faulty autovacuum process that is holding an index lock for an extended period, causing a locking conflict with global lock queries and ultimately leading to a prolonged connection timeout.",
          "score": 0.8
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is a misconfigured or improperly managed autovacuum process that acquired an exclusive lock on an index, preventing other database operations from accessing the metadata server and causing a prolonged locking conflict.",
          "score": 0.7
        }
      ],
      "avg_score": 0.833,
      "correct": true
    },
    {
      "id": "incident_008",
      "company": "Stack Exchange",
      "category": "Network",
      "ground_truth": "Enabling the 'StackEgg' feature globally caused a massive traffic spike to load balancers, effectively behaving like a self-inflicted DDoS.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is that a traffic spike caused by enabling the StackEgg feature flag, which was intended for testing purposes, resulted in an unintended self-inflicted Distributed Denial of Service (DDoS) attack on the system.",
          "score": 0.9
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is that the \"StackEgg\" feature flag, which was enabled for testing purposes, inadvertently triggered a self-inflicted Distributed Denial of Service (DDoS) attack on the system.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is likely that the StackEgg feature flag, which was enabled for testing purposes, inadvertently triggered a self-inflicted Distributed Denial of Service (DDoS) attack on the system.",
          "score": 0.7
        }
      ],
      "avg_score": 0.767,
      "correct": true
    },
    {
      "id": "incident_009",
      "company": "Facebook",
      "category": "Network",
      "ground_truth": "A routine maintenance audit command inadvertently disconnected all Facebook data centers from the Internet by withdrawing all BGP routes.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident appears to be a misconfigured or improperly implemented BGP (Border Gateway Protocol) route processing mechanism that unexpectedly withdrew all routes, triggering a global outage.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be a misconfiguration or incorrect implementation of the BGP (Border Gateway Protocol) route processing mechanism, which led to an unexpected withdrawal of all routes and subsequently caused a global outage.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a misconfigured or faulty BGP (Border Gateway Protocol) route processing mechanism that unexpectedly withdrew all routes, triggering a global outage and subsequent recovery efforts.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_010",
      "company": "Amazon",
      "category": "Infrastructure",
      "ground_truth": "Human error: A typo in a command intended to remove a few billing servers inadvertently removed a large set of critical S3 storage systems.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the S3 outage was a syntax error in an invalid command executed by the server removal process, which triggered a chain reaction of failures and cascading effects throughout the infrastructure.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of the AWS S3 outage was a syntax error in an invalid command executed by the server removal process, which triggered a cascade of failures and ultimately led to the massive deletion of S3 storage systems.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the S3 storage system deletion is likely a misconfigured or incorrectly executed command in the server's configuration files, possibly due to human error or a software bug.",
          "score": 0.7
        }
      ],
      "avg_score": 0.733,
      "correct": true
    },
    {
      "id": "incident_011",
      "company": "Homebrew",
      "category": "Security",
      "ground_truth": "A GitHub personal access token with write scopes was leaked in Jenkins build logs, allowing an attacker to push malicious commits.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is a lack of proper access control and security measures in place for Jenkins, specifically the failure to restrict write scopes on GitHub personal access tokens, allowing unauthorized access to push malicious changes to production repositories.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident is that a GitHub personal access token with write scopes was inadvertently leaked into the Jenkins build log, allowing an unknown IP address to push malicious commits to production branches.",
          "score": 0.9
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is that a GitHub personal access token with write scopes was inadvertently leaked in the Jenkins job build log, allowing unauthorized access to push malicious changes to production repositories.",
          "score": 1.0
        }
      ],
      "avg_score": 0.867,
      "correct": true
    },
    {
      "id": "incident_012",
      "company": "NPM",
      "category": "Configuration",
      "ground_truth": "Fastly/Varnish 'restart' command reset the request backend to the first one in the VCL list (Manta) instead of the intended pool.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a misconfigured VCL (Varnish Cache Configuration Language) that led to incorrect backend configuration, causing multiple downstream errors and ultimately resulting in the Manta backend failing to respond to requests.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be a misconfiguration in the Varnish Cache configuration, specifically with the backend configuration for the \"req.backend\" setting, which led to incorrect distribution and connection issues with the Manta backend.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is likely a misconfiguration or issue with the VCL (Varnish Cache Configuration Language) that caused the varnish service to incorrectly reload its backend configuration, leading to a cascade of errors and timeouts.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_013",
      "company": "Gitlab",
      "category": "Database",
      "ground_truth": "System administrator inadvertently deleted the production database directory during a replication lag crisis.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is a human error where an administrator attempted to wipe the staging database without properly verifying its contents and consequences, leading to the accidental deletion of the primary database directory.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident is a human error where an administrator attempted to wipe the staging database without properly verifying its contents or consequences, resulting in the accidental deletion of the primary database directory.",
          "score": 0.8
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is a human error where an administrator attempted to wipe the staging database without properly verifying that it was safe to do so, resulting in the accidental deletion of the primary database directory.",
          "score": 0.8
        }
      ],
      "avg_score": 0.8,
      "correct": true
    },
    {
      "id": "incident_014",
      "company": "CrowdStrike",
      "category": "Software",
      "ground_truth": "A buggy content update in Falcon sensor triggered an out-of-bounds memory read in the kernel driver, causing Windows BSOD.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a kernel driver bug in C-00000291*.sys that caused an out-of-bounds memory read, leading to system crashes and BSODs on millions of Windows machines.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be a critical vulnerability in the kernel driver C-00000291*.sys, which caused an out-of-bounds memory read and subsequent system crashes across millions of Windows machines.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident appears to be a critical vulnerability in the kernel driver C-00000291*.sys, which caused an out-of-bounds memory read and subsequently triggered a Windows BSOD on millions of machines, leading to widespread system crashes and disruptions to global transport and banking services.",
          "score": 0.0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_015",
      "company": "GitHub",
      "category": "Database",
      "ground_truth": "MySQL read replicas entered a semaphore deadlock during the final 'rename' step of a massive schema migration.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was that the MySQL read replicas entered a semaphore deadlock during the final \"rename\" step of a massive schema migration, which caused multiple replica nodes to crash and triggered a system-wide outage.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was that the MySQL read replicas entered a semaphore deadlock during the final \"rename\" step of a massive schema migration, which caused multiple replica nodes to crash and triggered a system-wide failure.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was that the MySQL read replicas entered a semaphore deadlock during the final \"rename\" step of a massive schema migration, which caused multiple replica nodes to crash and trigger a system-wide outage.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_016",
      "company": "Etsy",
      "category": "Database",
      "ground_truth": "Signed 32-bit integer overflow in database IDs caused operations to fail.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is that the database's primary key data type was not upgraded to support larger integer values, resulting in an ID overflow when a record with an extremely large ID (2147483647) was attempted to be inserted.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is that the database was using a signed 32-bit integer data type for its primary key, which had a maximum value limit of 2,147,483,647, causing an overflow when trying to insert records with IDs above this threshold.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is that the database was using a signed 32-bit integer data type for its primary key, which has a maximum value of 2,147,483,647, causing an overflow when trying to insert records with IDs above this value.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_017",
      "company": "Sentry",
      "category": "Database",
      "ground_truth": "PostgreSQL transaction ID wraparound forced the database into read-only mode to prevent data loss.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a failure to properly manage and maintain the database's write volume, leading to a cascade of issues including autovacuum failures, transaction ID wraparound, and ultimately, a Sentry outage.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be a failure in the database's ability to manage its write volume, leading to a cascade of errors and ultimately requiring the PostgreSQL server to be shut down for maintenance.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a failure in the autovacuum mechanism, which is unable to keep up with the high write volume, leading to a cascade of subsequent errors and ultimately resulting in a Sentry outage.",
          "score": 0.7
        }
      ],
      "avg_score": 0.733,
      "correct": true
    },
    {
      "id": "incident_018",
      "company": "Salesforce",
      "category": "Security",
      "ground_truth": "A security policy change inadvertently blocked access to resources beyond its intended scope, preventing logins.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the global login disruption was a misconfigured security policy update that inadvertently blocked legitimate user access, causing widespread authentication failures and disruptions to critical services.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the global login disruption was a misconfigured security policy update that inadvertently blocked legitimate user access to the admin console, causing a cascade of authentication failures and disruptions across multiple services.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the global login disruption was a misconfigured security policy update that inadvertently blocked legitimate user access to the admin console, causing widespread authentication failures and disruptions.",
          "score": 0.8
        }
      ],
      "avg_score": 0.933,
      "correct": true
    },
    {
      "id": "incident_019",
      "company": "Slack",
      "category": "Network",
      "ground_truth": "Network saturation in AWS transit gateways caused massive packet loss and connection drops.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is likely a sudden and unexpected increase in network traffic or load on the primary gateway, possibly due to a bursty application or service that was not properly scaled or provisioned for the expected user demand.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is likely a hardware or software failure in the primary AWS Transit Gateway, which resulted in an overload and subsequent packet loss, leading to disconnection issues for users.",
          "score": 0.8
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is likely a sudden and unexpected surge in network traffic, possibly caused by an unforeseen increase in user activity or a malicious attack, which overwhelmed the primary gateway's capacity, leading to packet loss, latency issues, and disconnections.",
          "score": 0.7
        }
      ],
      "avg_score": 0.733,
      "correct": true
    },
    {
      "id": "incident_020",
      "company": "Google",
      "category": "Software",
      "ground_truth": "A policy change with blank fields triggered a null pointer exception in Service Control, causing global crash loops.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a null pointer exception detected during policy replication, which led to a chain reaction of failures in various services and systems, ultimately resulting in a catastrophic system failure.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be a null pointer exception in the Policy Replication Service, which triggered a chain reaction of failures and ultimately led to a catastrophic system failure.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a null pointer exception detected during policy replication, which led to a chain reaction of failures in various services and systems.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_021",
      "company": "CircleCI",
      "category": "Security",
      "ground_truth": "Malware on an engineer's laptop stole a valid, 2FA-backed SSO session cookie, allowing unauthorized access to production systems.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a malware infection on an engineer's laptop, which compromised the security of the system and led to unauthorized access and potential data breaches.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a malware infection on an engineer's laptop, which compromised the security of the system and led to unauthorized access and potential data breaches.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a malware infection on an engineer's laptop, which compromised the security of the system and led to unauthorized access and potential data breaches.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_022",
      "company": "Discord",
      "category": "Infrastructure",
      "ground_truth": "Automated GCP migration of a Redis primary caused it to drop offline, triggering cascading failures in how Discord handles Redis failover.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is a severe data corruption issue during the automated GCP migration process, which led to the primary node dropping offline, causing a cascade of downstream failures in the Redis failover mechanism, web app, and ultimately resulting in widespread service disruptions.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident is a critical failure in the automated GCP migration process, which resulted in severe data corruption and subsequent cascading failures in the Redis failover mechanism, leading to widespread system disruptions.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a critical failure in the automated GCP migration process, which resulted in severe data corruption on the primary Redis node, leading to a cascade of downstream failures.",
          "score": 0.7
        }
      ],
      "avg_score": 0.733,
      "correct": true
    },
    {
      "id": "incident_023",
      "company": "Fastly",
      "category": "Software",
      "ground_truth": "An undiscovered software bug in Fastly's edge cloud was triggered by a valid customer configuration change, causing a global outage.",
      "runs": [
        {
          "run": 1,
          "prediction": "The ROOT CAUSE of the global outage affecting all services was an undiscovered software bug in Fastly's edge cloud triggered by a valid customer configuration change.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The ROOT CAUSE of the global outage was an undiscovered software bug in Fastly's edge cloud triggered by a valid customer configuration change.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The ROOT CAUSE of the global outage was an undiscovered software bug in Fastly's edge cloud triggered by a valid customer configuration change.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_024",
      "company": "Foursquare",
      "category": "Database",
      "ground_truth": "MongoDB ran out of memory due to a query pattern with low locality (fetching full history for every check-in), leading to catastrophic failure.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is that the system's available memory was insufficient to handle the workload, leading to a series of escalating warnings and eventual shutdown of the MongoDB database.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident is likely a failure in the system's memory management or resource allocation, possibly due to an unhandled memory leak or excessive memory usage by one or more applications or services.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is likely that the system was not configured or provisioned with sufficient memory resources for its MongoDB database, leading to a gradual accumulation of memory pressure and ultimately causing the database to run out of space.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_025",
      "company": "Flowdock",
      "category": "Infrastructure",
      "ground_truth": "Unexpected traffic spike due to remote work shift (COVID-19) overloaded backend components.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident appears to be a sudden and unexpected surge in traffic volume that overwhelmed the backend service's resources, leading to a cascade of failures and errors.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be a sudden and unexpected surge in traffic volume that overwhelmed the backend service's resources, leading to a cascade of failures and errors.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a sudden and unexpected surge in traffic volume that overwhelmed the backend service's resources, causing CPU utilization to exceed 80% and ultimately leading to system unresponsiveness.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_026",
      "company": "Heroku",
      "category": "Security",
      "ground_truth": "Leaked private tokens allowed attackers to access internal databases and private repositories.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a hardcoded private token 'GHI789' in the configuration file, which was not properly secured or validated.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a hardcoded private token 'GHI789' found in the configuration file, which was not properly secured or validated.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a hardcoded private token 'GHI789' found in the configuration file, which was not properly secured or validated.",
          "score": 0.7
        }
      ],
      "avg_score": 0.9,
      "correct": true
    },
    {
      "id": "incident_027",
      "company": "Basecamp",
      "category": "Network",
      "ground_truth": "Sustained DDoS attack on Basecamp's network infrastructure during a 100-minute window.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a sustained Distributed Denial-of-Service (DDoS) attack that overwhelmed the network infrastructure, causing a cascade of failures in critical services such as WebServer and DatabaseService.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is likely a misconfigured or inadequate rate limiting mechanism that failed to prevent excessive traffic from reaching the WebServer and DatabaseService, ultimately leading to a sustained Distributed Denial-of-Service (DDoS) attack.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a sustained Distributed Denial of Service (DDoS) attack that overwhelmed the network infrastructure, causing a chain reaction of failures in critical services such as WebServer and DatabaseService.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_028",
      "company": "Bitly",
      "category": "Security",
      "ground_truth": "Hosted source code repository contained plaintext credentials granting access to Bitly's database backups.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident appears to be a misconfigured database backup system that allowed unauthorized access, which was likely due to inadequate security controls or insufficient monitoring, leading to the exposure of sensitive credentials and data.",
          "score": 0.7
        },
        {
          "run": 2,
          "error": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
          "score": 0
        },
        {
          "run": 3,
          "error": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
          "score": 0
        }
      ],
      "avg_score": 0.233,
      "correct": false
    },
    {
      "id": "incident_029",
      "company": "Knight Capital",
      "category": "Software",
      "ground_truth": "An outdated code path (Power Peg) was inadvertently re-enabled during a production deployment, causing millions of erroneous trades.",
      "runs": [
        {
          "run": 1,
          "error": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
          "score": 0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was that the outdated code path re-enabled during deployment, which led to a cascade of errors and failures in the system.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was that the outdated code path re-enabled during the deployment process, which led to a cascade of errors and failures in the system.",
          "score": 0.7
        }
      ],
      "avg_score": 0.467,
      "correct": false
    },
    {
      "id": "incident_030",
      "company": "Keepthescore",
      "category": "Database",
      "ground_truth": "Developing on production: running a script locally that was connected to the production database and executing a drop command.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is likely a misconfigured or failed backup process, which resulted in the loss of critical data and triggered an alert that was not responded to by the DevOps team due to their holiday schedule.",
          "score": 0.4
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident appears to be a misconfigured or failed backup process, which resulted in the loss of critical data and triggered an alert that was not responded to by the DevOps team, ultimately leading to the incident being escalated to a high severity.",
          "score": 0.4
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is likely a misconfigured or faulty backup process that resulted in the failure to restore the database, leading to data loss and subsequent alert notifications.",
          "score": 0.4
        }
      ],
      "avg_score": 0.4,
      "correct": false
    },
    {
      "id": "incident_031",
      "company": "PagerDuty",
      "category": "Network",
      "ground_truth": "Common peering point failure across two 'independent' cloud deployments caused high latency and prevented the application from establishing quorum.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a persistent peering point failure, likely due to a network issue or connectivity problem that prevented both primary and secondary connections from being established between the clusters.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a persistent peering point failure on one or both of the primary connections between the clusters, resulting in high latency and ultimately preventing the establishment of quorum.",
          "score": 0.9
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a persistent peering point failure on one or both of the primary network connections, causing high latency and ultimately preventing the cluster from establishing quorum.",
          "score": 0.7
        }
      ],
      "avg_score": 0.8,
      "correct": true
    },
    {
      "id": "incident_032",
      "company": "Roblox",
      "category": "Infrastructure",
      "ground_truth": "Consul streaming feature enabled higher-than-expected load on BoltDB, leading to 73-hour global outage.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the global outage was that the Consul streaming feature was enabled without sufficient load testing and monitoring, leading to an unexpected surge in data volume that overwhelmed the BoltDB storage.",
          "score": 0.9
        },
        {
          "run": 2,
          "prediction": "The root cause of the global outage was an unhandled error in the Consul streaming service that led to a cascade of failures in the BoltDB storage and subsequent database corruption.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the global outage was an unhandled error in the Consul streaming service that led to a cascade of failures in the BoltDB storage and subsequent database corruption.",
          "score": 0.7
        }
      ],
      "avg_score": 0.767,
      "correct": true
    },
    {
      "id": "incident_033",
      "company": "Spotify",
      "category": "Software",
      "ground_truth": "Lack of exponential backoff in internal microservices caused retry storms and cascading service degradation.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause was a failure to implement an adequate load balancing strategy, leading to a cascading failure of the microservices due to insufficient capacity handling during a high traffic spike.",
          "score": 0.4
        },
        {
          "run": 2,
          "prediction": "The root cause was a failure to implement or configure an effective load balancing strategy, leading to a cascading failure of multiple services due to insufficient capacity handling high traffic spikes.",
          "score": 0.4
        },
        {
          "run": 3,
          "prediction": "The root cause was a sudden and unexpected high traffic spike that overwhelmed the system, causing a cascade of failures due to inadequate handling of the load.",
          "score": 0.7
        }
      ],
      "avg_score": 0.5,
      "correct": false
    },
    {
      "id": "incident_034",
      "company": "Slack",
      "category": "Database",
      "ground_truth": "Massive reconnection spike following a network glitch overwhelmed database capacity, leading to cascading connection failures.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident appears to be a network packet loss issue on the east coast data center, which led to a cascade of failures in database connections and ultimately caused service disruptions.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident appears to be a network packet loss issue at the east coast data center, which led to a cascade of failures in database connections and ultimately caused service disruptions.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a network packet loss on the east coast data center that caused a cascade of failures in the database connections, leading to service unavailability and high-severity alerts.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_035",
      "company": "Stack Exchange",
      "category": "Database",
      "ground_truth": "Primary SQL Server triggered a kernel bugcheck/crash, forcing sites into read-only mode and eventual outage.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a kernel bug that caused a system crash in the primary SQL server, leading to a cascade of failures across multiple systems and services.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause is that the primary SQL server failed due to a memory allocation failure reported by Windows, which triggered a system crash and subsequently caused all sites to go offline.",
          "score": 0.8
        },
        {
          "run": 3,
          "prediction": "The root cause is likely a hardware or software failure in the primary SQL server, possibly due to overheating, power supply issues, or a critical system component failure.",
          "score": 0.7
        }
      ],
      "avg_score": 0.767,
      "correct": true
    },
    {
      "id": "incident_036",
      "company": "Amazon",
      "category": "Infrastructure",
      "ground_truth": "Scaling the Kinesis front-end fleet exceeded OS thread limits due to memory-intensive configuration, breaking Cognito/Lambda.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is that the Lambda function was configured with a memory-intensive setting, causing it to exceed its thread limits and resulting in a cascade of failures across multiple services.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is that the Lambda function 'lambda-123' was configured with a memory-intensive setting, causing it to exceed its thread limit and fail to handle requests, leading to a cascade of failures across multiple services.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is that the Lambda function 'lambda-123' was configured with a memory-intensive architecture, which caused it to exceed its thread limit and subsequently crash due to OS thread limits exceeding capacity.",
          "score": 1.0
        }
      ],
      "avg_score": 0.933,
      "correct": true
    },
    {
      "id": "incident_037",
      "company": "Twilio",
      "category": "Database",
      "ground_truth": "Redis network partition caused resync storm, crashing the master and leaving billing in read-only mode, causing retry over-billing.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a network partition that caused a loss of connection between the Redis master instance and its slaves, resulting in a resync storm that led to the billing service entering read-only mode.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a network partition that caused a loss of connection between the Redis master instance and its slaves, leading to a resync storm that ultimately crashed the master instance.",
          "score": 0.8
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a network partition that caused a temporary disconnection between the Redis master instance and its slaves, leading to a resync storm that ultimately resulted in the master crashing and leaving the billing service in read-only mode.",
          "score": 0.7
        }
      ],
      "avg_score": 0.733,
      "correct": true
    },
    {
      "id": "incident_038",
      "company": "Zerodha",
      "category": "Software",
      "ground_truth": "Single order for 1M penny stock units generated 100k+ individual trades, overwhelming the Order Management System's limit.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is that the order management system was overwhelmed by a large and unexpected order for 1M penny stock units, causing it to exceed its maximum allowed trade volume limit.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is that the order management system was unable to handle a large and unexpected influx of trades, causing a cascade of failures in subsequent systems due to insufficient resources.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is that the order management system was unable to process a large and sudden influx of trades, exceeding its maximum allowed limit, which led to a cascade of subsequent errors and failures.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_039",
      "company": "Cloudflare",
      "category": "Software",
      "ground_truth": "Bug in ClickHouse query logic used for Bot Management generated duplicate features, exceeding fixed-size configuration buffers.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a bug in the ClickHouse query logic for the Bot Management Service, which led to resource constraints and timeouts.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a bug in the ClickHouse query logic for the Bot Management Service, which led to resource constraints and timeouts.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a bug in the ClickHouse query logic for the Bot Management Service, which caused resource constraints and timeouts, ultimately leading to configuration exceeding maximum capacity.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_040",
      "company": "Sentry",
      "category": "Security",
      "ground_truth": "Misconfigured S3 bucket permissions for backups lead to public exposure of sensitive data.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a missing permission on the backup bucket, which led to sensitive data being exposed publicly due to inadequate access controls.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a missing permission on the backup bucket, which led to the sensitive data being exposed publicly due to incorrect access controls.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a missing permission on the backup bucket, which allowed sensitive data to be exposed publicly due to inadequate access controls.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_041",
      "company": "Google",
      "category": "Software",
      "ground_truth": "The '/' character was accidentally checked into the malicious URL blacklist, causing every site on the internet to be flagged as harmful.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was an accidental check-in of the '/' character into the malicious URL blacklist, which triggered a cascade of errors and ultimately led to the system crashing.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the system failure was an accidental check-in of the '/' character into the malicious URL blacklist, which triggered a cascade of errors and ultimately led to all websites being flagged as malicious.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the system failure was an accidental check-in of the '/' character into the malicious URL blacklist, which triggered a cascade of errors and ultimately led to all websites being flagged as malicious.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_042",
      "company": "Google",
      "category": "Infrastructure",
      "ground_truth": "GCVE provisioning used a legacy option that created a 'fixed-term' contract, leading to automatic deletion of the environment upon expiry.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is that a legacy provisioning option with a fixed-term contract was used, which automatically deleted the environment when the contract term approached expiry.",
          "score": 0.9
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a configuration issue caused by using a legacy provisioning option for a fixed-term contract, which led to automatic environment deletion when the contract term approached expiry.",
          "score": 0.9
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a configuration issue with the legacy provisioning option for fixed-term contracts, which led to automatic environment deletion when the contract term approached expiry.",
          "score": 0.9
        }
      ],
      "avg_score": 0.9,
      "correct": true
    },
    {
      "id": "incident_043",
      "company": "TUI",
      "category": "Software",
      "ground_truth": "System upgrade caused female passengers with the title 'Miss' to be misidentified as children, using 35kg instead of 69kg for takeoff mass calculations.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a fundamental design flaw in the system's handling of passenger titles, specifically the incorrect assumption that female passengers with 'Miss' title are children, which led to a cascade of errors and failures throughout the flight planning process.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a flawed logic in the weight calculation algorithm that incorrectly identifies female passengers with 'Miss' title as children, leading to incorrect passenger weights being used in flight planning.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a flawed logic or data processing issue in the weight-calculator-service that incorrectly identifies female passengers with 'Miss' title as children in takeoff mass calculations.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_044",
      "company": "BBC",
      "category": "Database",
      "ground_truth": "Database backend overload caused throttling; services without local caching timed out and failed completely.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a sudden and unexpected surge in database load due to an unoptimized or poorly scaled caching mechanism that led to a cascade of downstream failures.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be a sudden and unexpected surge in database load due to an unanticipated increase in query volume or data retrieval requests, likely triggered by an external factor such as a sudden spike in user activity.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a sudden and unexpected surge in database load due to an unoptimized caching mechanism that led to a cascade of failures across multiple services.",
          "score": 0.7
        }
      ],
      "avg_score": 0.733,
      "correct": true
    },
    {
      "id": "incident_045",
      "company": "Buildkite",
      "category": "Database",
      "ground_truth": "Database capacity downgrade (cost saving) resulted in insufficient resources for peak load, causing cascading collapse.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is that the system was unable to scale its application and database resources in time to handle the increased traffic, leading to a cascade of failures due to insufficient resources.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was an insufficient allocation of resources (CPU, disk space) on the database server, which led to a cascade of failures in application scaling, database connection timeouts, and ultimately, a high availability issue.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was an insufficient allocation of database resources, which led to a cascade of failures in application scaling, high CPU usage, disk space exhaustion, and ultimately, a failure to scale up resources despite multiple attempts.",
          "score": 0.8
        }
      ],
      "avg_score": 0.733,
      "correct": true
    },
    {
      "id": "incident_046",
      "company": "NASA",
      "category": "Software",
      "ground_truth": "Priority inversion in VxWorks OS where a low-priority task held a mutex needed by a high-priority task, while a medium-priority task blocked the low-priority one.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a resource starvation issue due to an inadequate mutex (mutual exclusion) mechanism, leading to priority inversion and subsequent system instability caused by insufficient resources being allocated to high-priority tasks.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a resource starvation issue due to an inadequate mutex (mutual exclusion) mechanism, where low-priority tasks are holding onto critical resources for extended periods, causing high-priority tasks to be blocked and leading to priority inversion, system instability, and ultimately, a shutdown.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a resource starvation issue due to an inadequate mutex (mutual exclusion) mechanism, leading to priority inversion and system instability as high-priority tasks are unable to acquire necessary resources held by low-priority tasks.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_047",
      "company": "OpenAI",
      "category": "Database",
      "ground_truth": "Redis request/response queues became corrupted and out-of-sequence, revealing user data to other users.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a failure in Redis's data integrity, likely due to a configuration or connection issue that led to corrupted data being written to the database.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a failure in Redis's data integrity, likely due to a configuration issue or a bug that allowed corrupted data to be written to the database, which then propagated through the system and caused a cascade of errors leading to user data exposure.",
          "score": 0.8
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is a failure in the Redis connection handling, leading to corrupted data being sent to downstream services, resulting in out-of-sequence requests, user data exposure, and ultimately, a critical failure that requires emergency shutdown and containment.",
          "score": 0.8
        }
      ],
      "avg_score": 0.767,
      "correct": true
    },
    {
      "id": "incident_048",
      "company": "Grab",
      "category": "Infrastructure",
      "ground_truth": "Predictive autoscaling algorithm miscalculated the required capacity for a surge in demand, leading to resource exhaustion.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is that the predictive autoscaling algorithm miscalculated the required capacity for a surge in demand, leading to resource exhaustion.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident is that the predictive autoscaling algorithm miscalculated the required capacity for a surge in demand, leading to resource exhaustion.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is that the predictive autoscaling algorithm miscalculated the required capacity for a surge in demand, leading to resource exhaustion.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_049",
      "company": "Microsoft",
      "category": "Infrastructure",
      "ground_truth": "Simultaneous failure of multiple redundant cooling systems in a data center during a heatwave forced a shutdown of servers.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident appears to be a failure in the cooling system, which led to overheating and subsequent failures of multiple servers and power supplies, ultimately resulting in a loss of power and shutdown of all servers.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be a failure in the cooling system, which led to overheating and subsequent failures of multiple servers and power supplies, ultimately resulting in a loss of power and shutdown of all servers.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a failure in the cooling system, specifically a lack of redundancy or backup systems that could have prevented the cascade of failures and subsequent shutdowns.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_050",
      "company": "Atlassian",
      "category": "Software",
      "ground_truth": "Faulty script intended to decommission a legacy app inadvertently targeted active customer environments for deletion.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is that the script was not properly configured to handle missing dependencies, resulting in a failure to delete the legacy application across both development and production environments due to insufficient permissions.",
          "score": 0.4
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a lack of required permissions to delete the \"old-app\" in both the \"dev\" and \"prod\" environments, which prevented the script from successfully completing its execution.",
          "score": 0.4
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is that a critical dependency, \"dependency-1\", was not properly configured or updated in both the \"dev\" and \"prod\" environments, resulting in missing required permissions and ultimately causing the deletion process to fail.",
          "score": 1.0
        }
      ],
      "avg_score": 0.6,
      "correct": false
    },
    {
      "id": "incident_051",
      "company": "Heroku",
      "category": "Software",
      "ground_truth": "Incorrect deployment process caused new configuration variables not to be used by the code that required them, breaking git push.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is that the configuration variables required for the deployment were not properly set up in the environment before attempting to deploy new code, leading to a cascade of errors and ultimately causing the service to become unavailable.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident is that the configuration variables were not properly loaded from the environment due to a misconfiguration in the deployment process, which led to an incorrect application of new configuration variables and ultimately resulted in the service becoming unavailable.",
          "score": 0.9
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is that the configuration variables required for the deployment process were not properly set in the environment before attempting to deploy new code to the main branch.",
          "score": 0.8
        }
      ],
      "avg_score": 0.833,
      "correct": true
    },
    {
      "id": "incident_052",
      "company": "Microsoft",
      "category": "Configuration",
      "ground_truth": "A configuration change intended to improve performance across the storage service was deployed incorrectly, taking down Azure Storage globally.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is that a performance enhancement patch was deployed incorrectly, resulting in an invalid configuration file parameter value that propagated through the system and ultimately caused a global Azure Storage downtime.",
          "score": 0.9
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is that a performance enhancement patch was deployed incorrectly, resulting in an invalid configuration file value that triggered a cascade of errors leading to the global Azure Storage downtime.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is that a performance enhancement patch was deployed incorrectly, resulting in an invalid configuration file parameter value that triggered a cascade of failures throughout the Azure Storage system.",
          "score": 0.8
        }
      ],
      "avg_score": 0.9,
      "correct": true
    },
    {
      "id": "incident_053",
      "company": "Google",
      "category": "Network",
      "ground_truth": "An automated configuration generation tool removed all Google Compute Engine IP blocks from BGP announcements, leading to massive reachability issues.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a misconfiguration in the automated configuration generation tool that resulted in the removal of all Google Compute Engine IP blocks from BGP announcements, causing reachability issues across the network.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a misconfiguration or incorrect data in the automated configuration generation tool, specifically with regards to the Compute Engine IP blocks, which prevented the BGP announcements from being properly synchronized.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a misconfiguration or incorrect data in the automated configuration generation tool, which led to the removal of Compute Engine IP blocks from BGP announcements, causing reachability issues across the network.",
          "score": 0.9
        }
      ],
      "avg_score": 0.867,
      "correct": true
    },
    {
      "id": "incident_054",
      "company": "Google",
      "category": "Infrastructure",
      "ground_truth": "A new infrastructure feature triggered a latent issue within the internal network load balancer code in Europe regions.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a latent issue with the load balancer code that caused an internal network traffic spike, which led to a complete service unavailability in the Europe region.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a latent issue with the load balancer code that caused an internal network traffic spike, which ultimately led to the service becoming completely unavailable in the Europe region.",
          "score": 0.8
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a latent issue with the load balancer code that caused an internal network traffic spike, which led to a complete service unavailability in the Europe region.",
          "score": 0.7
        }
      ],
      "avg_score": 0.733,
      "correct": true
    },
    {
      "id": "incident_055",
      "company": "Reddit",
      "category": "Infrastructure",
      "ground_truth": "Critical Kubernetes cluster upgrade failed because node metadata changed between versions, breaking workload networking for over 5 hours.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a version mismatch between the control plane and worker nodes' node metadata, which caused incompatibility issues during the cluster upgrade process.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a version mismatch between the control plane and worker nodes' node metadata, which prevented the successful upgrade of the cluster's workload networking.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a version mismatch between the control plane and worker nodes' node metadata, which prevented the successful upgrade of the Kubernetes cluster.",
          "score": 0.8
        }
      ],
      "avg_score": 0.833,
      "correct": true
    },
    {
      "id": "incident_056",
      "company": "Strava",
      "category": "Database",
      "ground_truth": "Signed 32-bit integer limit hit on a primary key in the uploads database, causing all new uploads to fail.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a compatibility issue between the database schema and the primary key data type, which exceeded the size limit and prevented the successful update of the primary key data type.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a database schema design limitation that caused the primary key data type to exceed its size limit, resulting in a chain reaction of errors and failures across multiple services.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is that the primary key data type 'upload_id' exceeded its signed integer limit, causing a size limit exceeded error during database maintenance tasks.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_057",
      "company": "Stripe",
      "category": "Database",
      "ground_truth": "Incorrectly executed manual database operation (missing dependency) caused the Stripe API to go down for 90 minutes.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a sudden and unexpected spike in API request rate, which overwhelmed the Stripe API's ability to handle the increased load, leading to a cascade of subsequent errors.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a sudden and unexpected spike in API request rate, likely triggered by an external factor such as a marketing campaign or a surge in user activity.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a sudden and unexpected spike in API request rate, likely caused by an external factor such as a large-scale data migration or a concurrent system failure, which overwhelmed the Stripe API's database connection capabilities.",
          "score": 0.0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_058",
      "company": "Heroku",
      "category": "Software",
      "ground_truth": "An upstream 'apt' update broke pinned packages, causing write permission failures to /dev on customer dynos.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a failure in the apt package manager's ability to obtain necessary permissions to write to /dev, likely due to a configuration or dependency issue with the system.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a failure in the apt package manager's ability to obtain necessary permissions to write to /dev, likely due to a configuration issue or misconfiguration of the system.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is a failure in the apt package manager's ability to obtain necessary permissions to write to /dev, likely due to a configuration or dependency issue with the upstream apt update.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_059",
      "company": "Amazon",
      "category": "Software",
      "ground_truth": "Single corrupted bit in a gossip message caused distributed server state to overwhelm the S3 request processing fleet.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident appears to be a catastrophic failure of the system's load balancing and resource management mechanisms, likely due to an unforeseen spike in request volume or a sudden increase in memory usage that overwhelmed the system's capacity.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident appears to be a catastrophic failure of the system's load balancing and resource management mechanisms, which led to an uncontrolled escalation of requests across multiple services, resulting in a cascade of failures that ultimately threatened the entire system.",
          "score": 0.4
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident appears to be a sudden and unexpected surge in request volume, likely triggered by an external factor such as a large-scale data upload or a high-traffic event, which overwhelmed the system's capacity and led to a cascade of failures across multiple services.",
          "score": 0.4
        }
      ],
      "avg_score": 0.5,
      "correct": false
    },
    {
      "id": "incident_060",
      "company": "WhatsApp",
      "category": "Network",
      "ground_truth": "Configuration changes to Facebook's backbone routers also took down WhatsApp and Instagram globally.",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the WhatsApp service outage was a global misconfiguration change to Facebook's backbone routers that affected not only WhatsApp but also Instagram, causing a cascading failure.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a misconfiguration change to Facebook's backbone routers that cascaded and took down both WhatsApp and Instagram services globally.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a misconfiguration change made to Facebook's backbone router configuration that inadvertently caused a cascading failure affecting WhatsApp and Instagram services globally.",
          "score": 0.9
        }
      ],
      "avg_score": 0.967,
      "correct": true
    },
    {
      "id": "incident_061",
      "company": "Google",
      "category": "Software",
      "ground_truth": "Many changes to a rarely modified load balancer were applied through a very slow code path",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a slow code path issue in the load-balancer that prevented it from applying changes to its internal state and public addressing configurations in a timely manner.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident is a slow code path in the load-balancer that caused a cascade of failures and timeouts across multiple services, ultimately requiring a system-wide recovery.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is a slow and inefficient update process in the load-balancer's internal state, which led to a cascade of subsequent issues due to pending changes not being applied promptly.",
          "score": 0.7
        }
      ],
      "avg_score": 0.733,
      "correct": true
    },
    {
      "id": "incident_062",
      "company": "Google",
      "category": "Network",
      "ground_truth": "A failure of a component on a fiber path from one of the central US gateway campuses in Google\u2019s production backbone led to a decrease in available network bandwidth between the gateway and multiple e...",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a failed component on the fiber path US-W1, leading to high bandwidth loss and subsequent service disruptions.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a failed component on the fiber path US-W1, leading to high bandwidth loss and subsequent service disruptions.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a failed component on the fiber path US-W1, which led to high bandwidth loss and subsequent service disruptions.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_063",
      "company": "Knight Capital",
      "category": "Software",
      "ground_truth": "A combination of conflicting deployed versions and re-using a previously used bit caused a $460M loss",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a combination of conflicting deployed versions between services c and d, along with the re-use of previously used bits for deployment, which ultimately led to an unexpected error that caused a system failure.",
          "score": 0.0
        },
        {
          "run": 2,
          "error": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
          "score": 0
        },
        {
          "run": 3,
          "error": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
          "score": 0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_064",
      "company": "WebKit code repository",
      "category": "Configuration",
      "ground_truth": "The WebKit repository, a Subversion repository configured to use deduplication, became unavailable after two files with the same SHA-1 hash were checked in as test data, with the intention of implemen...",
      "runs": [
        {
          "run": 1,
          "error": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
          "score": 0
        },
        {
          "run": 2,
          "error": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
          "score": 0
        },
        {
          "run": 3,
          "error": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
          "score": 0
        }
      ],
      "avg_score": 0,
      "correct": false
    },
    {
      "id": "incident_065",
      "company": "Azure",
      "category": "Security",
      "ground_truth": "Certificates that were valid for one year were created",
      "runs": [
        {
          "run": 1,
          "error": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
          "score": 0
        },
        {
          "run": 2,
          "error": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
          "score": 0
        },
        {
          "run": 3,
          "error": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
          "score": 0
        }
      ],
      "avg_score": 0,
      "correct": false
    },
    {
      "id": "incident_066",
      "company": "Cloudflare",
      "category": "Network",
      "ground_truth": "Backwards time flow from tracking [the 27th leap second on 2016-12-31T23:59:60Z](https://hpiers",
      "runs": [
        {
          "run": 1,
          "error": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
          "score": 0
        },
        {
          "run": 2,
          "error": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
          "score": 0
        },
        {
          "run": 3,
          "error": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
          "score": 0
        }
      ],
      "avg_score": 0,
      "correct": false
    },
    {
      "id": "incident_067",
      "company": "Linux",
      "category": "Software",
      "ground_truth": "Leap second code was called from the timer interrupt handler, which held `xtime_lock`",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a deadlock in the xtime lock, which prevented the system from synchronizing its time correctly and ultimately led to a system crash.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident was a deadlock in the xtime lock, which prevented the system from synchronizing its time correctly and ultimately led to a system crash.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is a deadlock in the xtime lock, which prevented the system from synchronizing its time with external sources, leading to a cascade of subsequent errors and ultimately causing a system crash.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_068",
      "company": "Linux",
      "category": "Infrastructure",
      "ground_truth": "When a leap second occurred, `CLOCK_REALTIME` was rewound by one second",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was that the system's CLOCK_REALTIME clock was incorrectly rewound by one second due to a notification from the hrtimer, causing subsequent timer-related issues and excessive CPU usage.",
          "score": 0.9
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is that the system's CLOCK_REALTIME was incorrectly rewound by one second due to a notification from the hrtimer, causing a cascade of subsequent errors and failures.",
          "score": 0.9
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is that the system's CLOCK_REALTIME was incorrectly rewound by one second due to a notification from the hrtimer, causing subsequent timer-related issues and excessive CPU usage.",
          "score": 0.8
        }
      ],
      "avg_score": 0.867,
      "correct": true
    },
    {
      "id": "incident_069",
      "company": "Mozilla",
      "category": "Security",
      "ground_truth": "Most Firefox add-ons stopped working around May 4th 2019 when a certificate expired",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is that a privileged add-on's certificate had expired on May 4th, 2019, causing issues with certificate chain validation and ultimately leading to the disabling of add-ons.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is that a certificate, specifically 'add-on-certificate', had expired on May 4th, 2019, which caused issues with add-on validation and deployment.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is that a certificate used by the Firefox Addon Store had expired on May 4th, 2019, causing a chain validation failure and subsequent issues with add-on deployment and user data.",
          "score": 0.0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_070",
      "company": "Github",
      "category": "Database",
      "ground_truth": "The cascading nature of this scenario, there were not enough active read replicas to handle production requests which impacted the availability of core github services",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a lack of sufficient active read replicas in the MySQL cluster, leading to increased load and ultimately causing production requests to fail due to high latency.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a lack of sufficient active read replicas in the MySQL cluster, which resulted from an unknown issue with the schema migration process for the large MySQL table.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a lack of sufficient active read replicas, which led to a semaphore deadlock and ultimately resulted in high latency, production requests failing, and critical service unavailability.",
          "score": 0.8
        }
      ],
      "avg_score": 0.767,
      "correct": true
    },
    {
      "id": "incident_071",
      "company": "Heroku",
      "category": "Database",
      "ground_truth": "At 15:05 UTC on June 8, 2023, a database error occurred where a foreign key used a smaller data type than the primary key that it referenced",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is a data type mismatch in the database, which caused a primary key overflow and subsequent failures in creating new authorizations, triggering a full outage of the Heroku API.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a data type mismatch in the database, specifically with foreign keys, which caused a primary key overflow and subsequent cascading failures across multiple services.",
          "score": 0.8
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a data type mismatch in the database, specifically with foreign keys, which led to a primary key overflow and subsequent failure of customer deployment creation and authorization processes.",
          "score": 0.7
        }
      ],
      "avg_score": 0.733,
      "correct": true
    },
    {
      "id": "incident_072",
      "company": "Allegro",
      "category": "Infrastructure",
      "ground_truth": "The [Allegro](https://web",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a connection timeout in the task queue database, which led to a cascade of failures across multiple services and ultimately resulted in the entire platform becoming unavailable.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a connection timeout in the task queue database, which prevented the platform's asynchronous task processing from functioning correctly.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a connection timeout issue in the task queue database, which led to a cascade of failures across multiple services and ultimately resulted in the entire platform becoming unavailable.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_073",
      "company": "Amazon",
      "category": "Infrastructure",
      "ground_truth": "Human error",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a typo in the server list command, which led to the removal of critical S3 systems without confirmation, triggering a cascading failure that resulted in a full outage across the US-EAST-1 region.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a typo in the server list command that led to the removal of critical S3 systems without confirmation, resulting in a cascading failure across multiple dependent systems.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a typo in the server list command, which led to the removal of critical S3 systems without confirmation, ultimately resulting in a full outage across the US-EAST-1 region.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_074",
      "company": "Amazon",
      "category": "Infrastructure",
      "ground_truth": "Message corruption caused the distributed server state function to overwhelm resources on the S3 request processing fleet",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is a corrupted message received from service-a, which triggered a cascade of failures and ultimately led to the catastrophic failure of the S3 request processing fleet.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident is a corrupted message received from service-a, which triggered a cascade of failures and ultimately led to the catastrophic failure of the S3 request processing fleet.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is a failure in the service-a's message processing, resulting in corrupted messages being sent to the S3 request processing fleet, which overwhelmed the resources and led to a catastrophic failure.",
          "score": 0.8
        }
      ],
      "avg_score": 0.733,
      "correct": true
    },
    {
      "id": "incident_075",
      "company": "Amazon",
      "category": "Network",
      "ground_truth": "Human error during a routine networking upgrade led to a resource crunch, exacerbated by software bugs, that ultimately resulted in an outage across all US East Availability Zones as well as a loss of...",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is an unexpected software bug encountered during resource allocation, which caused a critical failure in the network upgrade service, leading to a widespread outage across all US East Availability Zones.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is an unexpected software bug encountered during resource allocation, which caused a cascade of failures leading to a critical outage across all US East Availability Zones.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is an unexpected software bug encountered during resource allocation in the network-upgrade-service, which caused a cascade of failures leading to a critical outage across all US East Availability Zones.",
          "score": 0.7
        }
      ],
      "avg_score": 0.733,
      "correct": true
    },
    {
      "id": "incident_076",
      "company": "Amazon",
      "category": "Infrastructure",
      "ground_truth": "Inability to contact a data collection server triggered a latent memory leak bug in the reporting agent on the storage servers",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a memory leak in the reporting agent process on storage server 'srv-123', which led to excessive memory consumption and ultimately caused the system failure.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a memory leak in the reporting agent on storage server 'srv-123', which led to excessive memory consumption and ultimately caused the system failure.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a memory leak in the reporting agent process on storage server 'srv-123', which led to excessive memory consumption and ultimately caused the system failure.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_077",
      "company": "Amazon",
      "category": "Infrastructure",
      "ground_truth": "Elastic Load Balancer ran into problems when \"a maintenance process that was inadvertently run against the production ELB state data\"",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the Elastic Load Balancer (ELB) failure appears to be a data synchronization issue that occurred during the nightly maintenance process, which likely resulted from an incorrect or incomplete update attempt by the ELB controller.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of the Elastic Load Balancer (ELB) failure is likely a misconfiguration or incorrect data synchronization process in the production ELB state data, which resulted in inconsistent data and ultimately led to the loss of control plane connectivity.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the Elastic Load Balancer (ELB) failure appears to be a data synchronization issue that occurred during the nightly maintenance process, which likely resulted in an inconsistent state being updated, leading to a cascade of subsequent errors and failures.",
          "score": 0.8
        }
      ],
      "avg_score": 0.767,
      "correct": true
    },
    {
      "id": "incident_078",
      "company": "Amazon",
      "category": "Network",
      "ground_truth": "A \"network disruption\" caused metadata services to experience load that caused response times to exceed timeout values, causing storage nodes to take themselves down",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is likely a misconfigured or inadequate load balancing mechanism that caused the metadata service and storage nodes to become overwhelmed with requests, leading to excessive CPU usage, timeouts, and eventual self-shutdowns.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is likely a misconfigured or overloaded storage system, leading to high CPU usage and response time issues in the metadata service, which caused it to take itself down due to excessive load.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is likely a misconfigured or inadequate load balancing mechanism that caused an uneven distribution of workload across the storage nodes, leading to excessive CPU usage and subsequent self-shutdowns.",
          "score": 0.4
        }
      ],
      "avg_score": 0.6,
      "correct": false
    },
    {
      "id": "incident_079",
      "company": "Amazon",
      "category": "Infrastructure",
      "ground_truth": "Scaling the front-end cache fleet for Kinesis caused all of the servers in the fleet to exceed the maximum number of threads allowed by an operating system configuration",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was that the scaling of the Kinesis cache fleet for the front-end exceeded the thread count limit set by the OS configuration.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was that the scaling of the Kinesis cache fleet for front-end services caused the thread count limit to be exceeded by the OS configuration, leading to a cascade of downstream failures.",
          "score": 0.9
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was that the scaling of the Kinesis cache fleet for front-end services exceeded the thread count limit set by the operating system configuration.",
          "score": 0.9
        }
      ],
      "avg_score": 0.933,
      "correct": true
    },
    {
      "id": "incident_080",
      "company": "Amazon",
      "category": "Network",
      "ground_truth": "At 7:30 AM PST, an automated activity to scale capacity of one of the AWS services hosted in the main AWS network triggered an unexpected behavior from a large number of clients inside the internal ne...",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is likely a misconfigured or inadequate network infrastructure that resulted in packet loss, buffer overflows, and high CPU utilization due to insufficient resources, ultimately leading to service degradation and alerting the incident team.",
          "score": 0.5
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be a sudden and unexpected surge in traffic volume for service 'service-x', which overwhelmed the network infrastructure, causing packet loss, buffer overflows, and congestion.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is likely a misconfigured or inadequate network infrastructure that resulted in packet loss, buffer overflows, and queue overflows due to insufficient resources, leading to a cascade of failures across multiple services.",
          "score": 0.7
        }
      ],
      "avg_score": 0.633,
      "correct": false
    },
    {
      "id": "incident_081",
      "company": "AppNexus",
      "category": "Database",
      "ground_truth": "A double free revealed by a database update caused all \"impression bus\" servers to crash simultaneously",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a database connection timeout, likely due to a configuration or resource issue that caused multiple instances of the service to fail simultaneously.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a failure in the database connection, likely due to a misconfiguration or issue with the underlying database system.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a failure in the database connection, which led to a cascade of errors and failures across multiple servers due to shared dependencies or interconnectivity issues.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_082",
      "company": "AT&T",
      "category": "Network",
      "ground_truth": "A bad line of C code introduced a race hazard which in due course collapsed the phone network",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a race hazard in the C code used by the system, which led to a cascade of failures across multiple network switches and services.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a race hazard in the C code used by the network management system, which led to a cascade of failures and ultimately resulted in a critical system failure across the entire network.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a race hazard in the C code used by the network management system, which led to a cascade of failures and ultimately resulted in a critical system failure across the entire network.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_083",
      "company": "Atlassian",
      "category": "Infrastructure",
      "ground_truth": "On Tuesday, April 5th, 2022, starting at 7:38 UTC, 775 Atlassian customers lost access to their Atlassian products",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident appears to be a failure in the underlying infrastructure or connectivity issues that prevented services from establishing connections with each other and with external systems, leading to a cascade of failures across multiple services.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be a failure in the underlying infrastructure, likely related to network connectivity or a hardware issue, which led to a cascade of failures across multiple services and ultimately required significant recovery efforts.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a failure in the underlying infrastructure, specifically a problem with the API connection or database connectivity that cascaded through multiple services and ultimately led to a critical failure.",
          "score": 0.0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_084",
      "company": "Basecamp",
      "category": "Network",
      "ground_truth": "Basecamp's network was under a DDoS attack during a 100-minute window on March 24, 2014",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the DDoS attack was likely a misconfigured firewall rule that incorrectly flagged legitimate traffic from IP 192.168.1.100 as malicious, leading to the blocking of normal traffic and subsequent network congestion.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the DDoS attack was likely a misconfigured firewall rule that incorrectly flagged legitimate traffic from IP 192.168.1.100 as malicious, leading to the blocking of normal traffic and subsequent network congestion.",
          "score": 0.3
        },
        {
          "run": 3,
          "prediction": "The root cause of the DDoS attack was likely a misconfigured firewall rule that incorrectly flagged a legitimate IP address as malicious, leading to unnecessary blocking and network congestion.",
          "score": 0.0
        }
      ],
      "avg_score": 0.1,
      "correct": false
    },
    {
      "id": "incident_085",
      "company": "Basecamp",
      "category": "Database",
      "ground_truth": "In November 2018 a database hit the integer limit, leaving the service in read-only mode",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is that a database schema upgrade was not performed before reaching the maximum allowed value for the \"max_id\" field, leading to an integer limit exceedance.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is that a database schema upgrade was not performed before reaching the maximum allowed integer value, leading to an integer limit exceeded error.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is that a database upgrade was not performed before reaching the maximum allowed value for max_id, leading to an integer limit exceeding issue.",
          "score": 0.0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_086",
      "company": "BBC Online",
      "category": "Database",
      "ground_truth": "In July 2014, BBC Online experienced a very long outage of several of its popular online services including the BBC iPlayer",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident appears to be an unexpected surge in database load due to a high volume of requests from multiple services, which overwhelmed the database's capacity and led to a cascade of failures across various online services.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a sudden and unexpected surge in database load due to an unknown external factor, likely caused by a large number of concurrent requests from users accessing BBC iPlayer services simultaneously.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a high load on the database backend, which led to a cascade of failures across multiple online services due to cache misses, request timeouts, and throttling.",
          "score": 0.0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_087",
      "company": "Bintray",
      "category": "Software",
      "ground_truth": "In July 2017 several malicious Maven packages were included in JCenter with an impersonation attack",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is a security vulnerability in JCenter's repository management system that allowed an unknown user to upload malicious packages, which were later discovered and removed by the Bintray Security Team.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident is a security vulnerability in JCenter's repository management system, allowing unknown users to upload malicious packages that were later injected into multiple Android apps with compromised dependencies.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is a security vulnerability in JCenter's repository management system, allowing malicious packages to be uploaded and distributed to multiple apps, compromising the security of at least 10 affected apps.",
          "score": 0.0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_088",
      "company": "Bitly",
      "category": "Security",
      "ground_truth": "Hosted source code repo contained credentials granting access to bitly backups, including hashed passwords",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is that the hosted source code repository contained credentials granting access to Bitly backups, including hashed passwords.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is that the hosted source code repository contained credentials granting access to Bitly backups, including hashed passwords.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was that the hosted source code repository contained credentials granting access to Bitly backups, including hashed passwords.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_089",
      "company": "BrowserStack",
      "category": "Security",
      "ground_truth": "An old prototype machine with the [Shellshock](https://en",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is that a Shellshock vulnerability was detected on the prototype machine, which allowed unauthorized access to secret keys and ultimately led to a potential security breach.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is that a Shellshock vulnerability was detected on the prototype machine, which allowed unauthorized access to sensitive secret keys and ultimately led to a potential security breach.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is that a Shellshock vulnerability was detected on the prototype machine, which allowed unauthorized access to sensitive information and ultimately led to a potential security breach.",
          "score": 1.0
        }
      ],
      "avg_score": 0.8,
      "correct": true
    },
    {
      "id": "incident_090",
      "company": "Buildkite",
      "category": "Database",
      "ground_truth": "Database capacity downgrade in an attempt to minimise AWS spend resulted in lack of capacity to support Buildkite customers at peak, leading to cascading collapse of dependent servers",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is that the automated capacity reduction measures implemented to minimize AWS spend resulted in a sudden and unexpected increase in database utilization, leading to a cascade of failures and unavailability of critical services.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident is that the capacity reduction measures implemented to minimize AWS spend resulted in a sudden and unexpected increase in database utilization, leading to a cascade of failures and unavailability of critical services.",
          "score": 0.9
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is that the automated capacity reduction measures implemented by Buildkite to minimize AWS spend resulted in a sudden and unexpected increase in database utilization, leading to a cascade of failures and unavailability of critical services.",
          "score": 0.8
        }
      ],
      "avg_score": 0.833,
      "correct": true
    },
    {
      "id": "incident_091",
      "company": "Bungie",
      "category": "Infrastructure",
      "ground_truth": "Side effects of a bug fix for wrong timestamps causes data loss; server misconfiguration for the hotfix causes the data loss to reappear in several servers in a following update",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a misconfiguration of the hotfix deployment process, which led to incorrect timestamp settings being applied across all servers, resulting in data loss and subsequent errors.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a misconfiguration of the hotfix deployment process, which led to data loss and subsequent errors across multiple servers.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a misconfiguration of the hotfix deployment process, which led to data loss and subsequent errors across multiple servers.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_092",
      "company": "CCP Games",
      "category": "Infrastructure",
      "ground_truth": "A problematic logging channel caused cluster nodes dying off during the cluster start sequence after rolling out a new game patch",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is likely a failure to properly authenticate database connections, which led to a cascade of failures across the cluster, ultimately resulting in all nodes becoming unavailable.",
          "score": 0.4
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a database connection failure that led to a cascade of failures across multiple services and ultimately resulted in the entire cluster becoming unavailable.",
          "score": 0.4
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a failure in the logging-channel service, which led to a cascade of failures across other services due to its critical role in log collection and aggregation.",
          "score": 0.7
        }
      ],
      "avg_score": 0.5,
      "correct": false
    },
    {
      "id": "incident_093",
      "company": "CCP Games",
      "category": "Memory",
      "ground_truth": "Documents a Stackless Python memory reuse bug that took years to track down",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a Stackless Python memory reuse bug identified and tracked to a specific version.",
          "score": 0.9
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a Stackless Python memory reuse bug identified and tracked to a specific version.",
          "score": 0.9
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a Stackless Python memory reuse bug identified and tracked to a specific version.",
          "score": 0.9
        }
      ],
      "avg_score": 0.9,
      "correct": true
    },
    {
      "id": "incident_094",
      "company": "Chef.io",
      "category": "Network",
      "ground_truth": "Intermittent unresponsiveness and increased latency",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a temporary network congestion issue that caused high latency and unresponsiveness to client requests, which was exacerbated by the distributed system's inability to detect and recover from node failures in a timely manner.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a misconfigured or failing network connection that resulted in intermittent and prolonged high latency issues, causing the service to fail its health checks and eventually terminate.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a misconfiguration or issue with the distributed system's load balancing or routing mechanism that caused requests to be routed to an unresponsive node, leading to repeated failed health checks and ultimately resulting in service termination.",
          "score": 0.7
        }
      ],
      "avg_score": 0.8,
      "correct": true
    },
    {
      "id": "incident_095",
      "company": "CircleCI",
      "category": "Infrastructure",
      "ground_truth": "A GitHub outage and recovery caused an unexpectedly large incoming load",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident appears to be an unexpected spike in load that overwhelmed the queue system of CircleCI, causing a cascade of failures and timeouts across multiple services.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was an unexpected spike in load that caused a queue slowdown in service-c, which led to a cascade of failures across multiple services, including CircleCI's queue system and database connection timeouts.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be an unexpected spike in load that overwhelmed the queue system of CircleCI, causing a cascade of failures across multiple services.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_096",
      "company": "CircleCI",
      "category": "Security",
      "ground_truth": "By January 4, 2023, our internal investigation had determined the scope of the intrusion by the unauthorized third party and the entry path of the attack",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is that a new employee, engineer-123, was not properly vetted or screened for malware before being granted access to company systems, allowing them to introduce malicious software into the production environment.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the IT incident was a previously undetected malware infection on an employee's laptop, which was introduced through a new hire process that did not include adequate security screening or training.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a previously undetected vulnerability in the organization's antivirus software that allowed malware to be introduced into the system, which was then exploited by an authorized employee.",
          "score": 0.0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_097",
      "company": "Cloudflare",
      "category": "Infrastructure",
      "ground_truth": "A parser bug caused Cloudflare edge servers to return memory that contained private information such as HTTP cookies, authentication tokens, HTTP POST bodies, and other sensitive data",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the memory leak incident is a bug in the parser code that allows sensitive HTTP cookies and tokens to be returned with responses, potentially exposing user data.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the memory leak incident is a bug or vulnerability in the parser code that allows sensitive HTTP POST bodies and cookies/tokens to be returned with responses, potentially exposing user data.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the memory leak incident is a vulnerability in the parser code that allows sensitive HTTP cookies and tokens to be returned with responses, potentially exposing user data.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_098",
      "company": "Cloudflare",
      "category": "Software",
      "ground_truth": "A single waf rule that contained a poorly written regular expression that ended up creating excessive backtracking",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is an excessively complex and resource-intensive WAF (Web Application Firewall) rule that caused CPU exhaustion, leading to a cascade of downstream failures in other services.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is an excessively complex and resource-intensive WAF (Web Application Firewall) rule that was not properly optimized or tested, leading to CPU exhaustion and subsequent downtime.",
          "score": 0.8
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is an excessively complex and resource-intensive WAF (Web Application Firewall) rule that caused CPU exhaustion, leading to a cascade of failures across multiple services.",
          "score": 0.7
        }
      ],
      "avg_score": 0.733,
      "correct": true
    },
    {
      "id": "incident_099",
      "company": "CrowdStrike",
      "category": "Memory",
      "ground_truth": "A bug in the content validator in the deployment stage",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a bug in the content validator, which led to a chain reaction of failures and ultimately resulted in widespread system crashes.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a bug in the content validator, which led to a chain reaction of validation failures, system crashes, and widespread errors that ultimately required manual rollback, patching, and remediation procedures.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a bug in the content validator, which led to a cascade of failures in deployment validation, system crashes, and ultimately, widespread BSOD errors.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_100",
      "company": "Datadog",
      "category": "Network",
      "ground_truth": "After an automatic upgrade, all network rules were removed and caused a 24h duration outage of all their Cilium protected Kubernetes clusters in all their regions and cloud providers",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the network outage was that all network rules were removed during an automatic upgrade, which led to a loss of connectivity for Cilium protected Kubernetes clusters in all regions and cloud providers.",
          "score": 0.9
        },
        {
          "run": 2,
          "prediction": "The root cause of the network outage was that all network rules were removed during an automatic upgrade, causing a 24h duration outage of Cilium protected Kubernetes clusters in all regions and cloud providers.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the network outage was that all network rules were removed during an automatic upgrade, which caused a 24-hour duration outage of Cilium protected Kubernetes clusters in all regions and cloud providers.",
          "score": 1.0
        }
      ],
      "avg_score": 0.967,
      "correct": true
    },
    {
      "id": "incident_101",
      "company": "Discord",
      "category": "Memory",
      "ground_truth": "Internal queues filling up",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is a memory leak in the mem-service, which has caused an accumulation of internal queues that are not being properly cleaned up or reclaimed, leading to a rapid increase in queue size and subsequent failures.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is an unhandled memory leak in the mem-service, which has led to a rapid increase in internal queue size, causing subsequent failures and cascading incidents.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is an uncontrolled memory leak in the mem-service, which has led to a rapid increase in internal queue size and subsequent failures in other services.",
          "score": 0.7
        }
      ],
      "avg_score": 0.9,
      "correct": true
    },
    {
      "id": "incident_102",
      "company": "Discord",
      "category": "Database",
      "ground_truth": "\"At approximately 14:01, a Redis instance acting as the primary for a highly-available cluster used by Discord's API services was migrated automatically by Google\u2019s Cloud Platform",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is a misconfigured Redis primary instance that was not properly maintained during the scheduled maintenance, resulting in a node not being available for migration and subsequent failover.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident is a misconfigured Redis primary instance that was not properly maintained during the scheduled maintenance window, resulting in a node not being available for migration and subsequent failover issues.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is a misconfigured Redis primary instance that was scheduled for maintenance during peak usage hours, causing a cascade of failures and subsequent service restarts.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_103",
      "company": "Dropbox",
      "category": "Database",
      "ground_truth": "This postmortem is pretty thin and I'm not sure what happened",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause is that the scheduled OS upgrade on node-1 caused a cascade failure due to unaddressed high disk usage issues, which were not resolved before the upgrade, ultimately leading to database connection failures and subsequent node failures.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause is that the scheduled OS upgrade on node-1 was initiated without sufficient disk space verification, leading to high disk usage and subsequent failure of the OS upgrade process.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a disk space issue, likely due to insufficient disk space or poor disk health on one or both nodes (node-1 and node-2), which prevented the OS upgrade from completing successfully and subsequently caused database connection failures.",
          "score": 0.0
        }
      ],
      "avg_score": 0.333,
      "correct": false
    },
    {
      "id": "incident_104",
      "company": "Duo",
      "category": "Database",
      "ground_truth": "A request queue overloading the existing, insufficient database capacity",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was an unexpected surge in high-volume login requests that overwhelmed the request queue, causing a cascade of subsequent failures and alerts due to inadequate database capacity planning.",
          "score": 0.9
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be an unexpected surge in high-volume login requests that overwhelmed the request queue, leading to a cascade of subsequent failures and alerts.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be an unexpected surge in high-volume login requests that overwhelmed the request queue, leading to a cascade of subsequent errors and failures.",
          "score": 0.7
        }
      ],
      "avg_score": 0.767,
      "correct": true
    },
    {
      "id": "incident_105",
      "company": "Epic Games",
      "category": "Infrastructure",
      "ground_truth": "Extreme load (a new peak of 3",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the system failure was an unexpected and unanticipated surge in concurrent user traffic, which exceeded the system's capacity and led to a cascade of failures across multiple services.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the system failure was an unexpected surge in concurrent user traffic, which exceeded the system's capacity and caused a cascade of failures across multiple services.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the system failure was an unexpected surge in concurrent user traffic that exceeded the system's capacity, causing a cascade of failures across multiple services and ultimately leading to a critical system failure.",
          "score": 0.7
        }
      ],
      "avg_score": 0.9,
      "correct": true
    },
    {
      "id": "incident_106",
      "company": "European Space Agency",
      "category": "Infrastructure",
      "ground_truth": "An overflow occurred when converting a 16-bit number to a 64-bit numer in the Ariane 5 intertial guidance system, causing the rocket to crash",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause is likely due to an overflow error in the conversion process, which triggered a cascade of subsequent errors and ultimately led to the rocket crash.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause is likely due to an integer overflow in the conversion process, where a 16-bit number was converted to a 64-bit numerical value, exceeding the maximum limit and triggering subsequent errors.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is likely an overflow error in the conversion process, which caused an invalid data value to be interpreted as valid, ultimately leading to a critical system failure and rocket crash.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_107",
      "company": "Elastic",
      "category": "Infrastructure",
      "ground_truth": "Elastic Cloud customers with deployments in the AWS eu-west-1 (Ireland) region experienced severely degraded access to their clusters for roughly 3 hours",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a high CPU usage issue in the region logger process that led to its crash, resulting in a cascade of failures and eventual manual intervention.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a high CPU usage issue that led to the region logger process crashing, causing a cascade of failures and eventual manual intervention.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a high CPU usage issue in the region logger process that led to its crash, resulting in a cascade of failures and eventual manual intervention.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_108",
      "company": "Elastic",
      "category": "Infrastructure",
      "ground_truth": "Elastic Cloud customers with deployments in the AWS us-east-1 region experienced degraded access to their clusters",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a potential misconfiguration in the AWS region settings that led to network unavailability and subsequent cluster connections issues.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a misconfiguration in the AWS regions that led to network unavailability and subsequent degradation of cluster connections.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a misconfiguration in the AWS regions that led to network unavailability and subsequent degradation of cluster connections.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_109",
      "company": "ESLint",
      "category": "Infrastructure",
      "ground_truth": "On July 12th, 2018, an attacker compromised the npm account of an ESLint maintainer and published malicious packages to the npm registry",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a compromised npm account belonging to an ESLint maintainer, which allowed malicious packages to be published to the registry.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a compromised npm account belonging to an ESLint maintainer, which allowed malicious packages to be published to the registry.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a compromised npm account belonging to an ESLint maintainer, which allowed malicious packages to be published to the registry.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_110",
      "company": "Etsy",
      "category": "Database",
      "ground_truth": "First, a deploy that was supposed to be a small bugfix deploy also caused live databases to get upgraded on running production machines",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a design flaw in the database schema, specifically an integer overflow vulnerability in the \"ids\" table, which was not properly addressed during the deployment of version 1.2.3, leading to repeated failures and cascading effects on production servers.",
          "score": 0.1
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a design flaw in the database schema, specifically an integer overflow vulnerability in the \"ids\" table that was not properly addressed during the deployment of version 1.2.3, leading to repeated failures and cascading consequences throughout the production environment.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a design flaw in the database schema, specifically an integer overflow issue on the \"ids\" table, which was not properly handled during the deployment of version 1.2.3, leading to repeated failures and subsequent rollbacks.",
          "score": 1.0
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_111",
      "company": "Fastly",
      "category": "Configuration",
      "ground_truth": "An undiscovered software bug that surfaced on june 8 when it was triggered by a valid customer configuration change",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was an unidentified software bug that caused a cascade of failures across multiple services, leading to a complete system failure.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was an unidentified software bug that caused a cascade of failures across multiple services, leading to a complete system failure.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was an unidentified software bug that caused a cascade of failures across multiple services, leading to a complete system failure.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_112",
      "company": "Flowdock",
      "category": "Database",
      "ground_truth": "Flowdock instant messaging was unavailable for approx 24 hrs between April 21-22 2020",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause is that the application's inability to handle high incoming traffic led to a prolonged database connection failure, resulting in a hung database and subsequent loss of user data.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause is that the application's inability to handle high incoming traffic led to a prolonged database connection failure, resulting in a hung database and subsequent loss of user data.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause is that the application's inability to handle high incoming traffic led to a prolonged database connection failure, resulting in a hung database and subsequent loss of user data.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_113",
      "company": "Foursquare",
      "category": "Database",
      "ground_truth": "A a query pattern that involved a read-load with low levels of locality (each user check-in caused a read of all check-ins for the user's history, and records were 300 bytes with no spatial locality, meaning that most of the data pulled in from each page was unnecessary)",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause is that the system's query processing and optimization were not adequately handled, leading to repeated memory allocation failures and high load conditions, ultimately resulting in multiple out-of-memory errors and failovers.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident is that the high load on the MongoDB service was not adequately addressed, leading to repeated failures and a cascade of subsequent issues with other services.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause is that the system was unable to handle a large and complex query pattern from the Foursquare service, leading to repeated instances of high load, out-of-memory errors, and subsequent failovers.",
          "score": 0.0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_114",
      "company": "Gentoo",
      "category": "Infrastructure",
      "ground_truth": "An entity gained access to the Gentoo GitHub organization, removed access to all developers and started adding commits in various repositories",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident appears to be a security vulnerability or misconfiguration in the GitHub organization's authentication system, allowing an unauthorized user to gain access and make commits without being detected until it was too late.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident appears to be a security vulnerability or weakness in the GitHub organization's authentication system, allowing an unauthorized user to gain access and make commits without being detected until it was too late.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident appears to be a security vulnerability or weakness in the GitHub organization's authentication system, allowing an unauthorized user to gain access and make changes to multiple repositories.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_115",
      "company": "GitHub",
      "category": "Infrastructure",
      "ground_truth": "On February 28th 2018, GitHub experienced a DDoS attack, hitting the website with 1",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a Distributed Denial-of-Service (DDoS) attack, which overwhelmed the system's resources and caused service availability to drop below 50%.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a Distributed Denial of Service (DDoS) attack that overwhelmed the API gateway and database services, causing a significant spike in traffic from an unknown source.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a Distributed Denial of Service (DDoS) attack that overwhelmed the system's resources, causing excessive traffic and service unavailability.",
          "score": 0.0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_116",
      "company": "Gitlab",
      "category": "Infrastructure",
      "ground_truth": "After the primary locked up and was restarted, it was brought back up with the wrong filesystem, causing a global outage",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is likely a misconfiguration or incorrect setup of the filesystem, specifically the \"newfs\" type, which led to a cascade of failures across multiple services and ultimately triggered an emergency recovery procedure.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be a configuration issue related to the filesystem, specifically an incorrect mount of the \"newfs\" filesystem type, which led to a cascade of failures across multiple services and ultimately resulted in a global outage.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a configuration issue related to an incorrect filesystem mount, which led to a cascade of errors and failures across multiple services, ultimately resulting in a global outage.",
          "score": 0.7
        }
      ],
      "avg_score": 0.833,
      "correct": true
    },
    {
      "id": "incident_117",
      "company": "Gitlab",
      "category": "Database",
      "ground_truth": "Influx of requests overloaded the database, caused replication to lag, tired admin deleted the wrong directory, six hours of data lost",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this IT incident is that the directory deletion by the admin-user led to a disk space issue, which caused replication to fail and ultimately resulted in six hours of data loss.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of this IT incident is a misconfigured log rotation policy that led to disk space issues, resulting in directory deletion, replication failure, and ultimately, six hours of data loss.",
          "score": 0.4
        },
        {
          "run": 3,
          "prediction": "The root cause of this IT incident is a mismanaged directory deletion, which led to a cascade of subsequent failures in data replication, disk space issues, and ultimately, a critical failure of the overloaded database.",
          "score": 0.8
        }
      ],
      "avg_score": 0.633,
      "correct": false
    },
    {
      "id": "incident_118",
      "company": "Google",
      "category": "Infrastructure",
      "ground_truth": "A mail system emailed people more than 20 times",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is that the batch email job was not designed or implemented as a non-atomic operation, leading to a failure in sending emails to all users simultaneously and resulting in some users being sent multiple emails.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is that the cron-service's non-atomic operation, which attempts to send emails to a large number of users in batches, failed due to exceeding the maximum allowed attempts, causing a cascade of subsequent errors and alerts.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is that the batch email job was not designed or implemented as a non-atomic operation, leading to a situation where multiple attempts were made to send emails to users in rapid succession, exceeding the maximum allowed attempts and resulting in errors.",
          "score": 0.0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_119",
      "company": "Google",
      "category": "Memory",
      "ground_truth": "A global quota that applies to filestore",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a sudden and unexpected increase in CPU usage on the internal service, which led to a rapid escalation of API request limits, throttling, and ultimately, a global quota exceeded issue.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be an unexpected surge in CPU usage by the internal service, which led to a rapid increase in API request limits being exceeded, ultimately resulting in throttling and subsequent failures across multiple services.",
          "score": 0.4
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is that the filestore-service exceeded its API request limit, leading to throttling and subsequent failures in managing GCP projects, console access, and gcloud operations.",
          "score": 0.7
        }
      ],
      "avg_score": 0.6,
      "correct": false
    },
    {
      "id": "incident_120",
      "company": "Google",
      "category": "Network",
      "ground_truth": "The Google Meet Livestream feature experienced disruptions that caused intermittent degraded quality of experience for a small subset of viewers, starting 25 October 2021 0400 PT and ending 26 October...",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident appears to be a network connectivity issue that resulted in packet loss and high latency, which ultimately led to video playback interruptions for users, causing a cascade of downstream errors and alerts.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be a network connectivity issue, specifically packet loss and high latency, which led to intermittent video playback interruptions and rebuffering issues for users, ultimately causing a critical failure in the livestream service.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a network connectivity issue, specifically packet loss and high latency, which led to intermittent video playback interruptions for users, ultimately triggering alerts and notifications that required manual investigation by the support team.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_121",
      "company": "Google",
      "category": "Software",
      "ground_truth": "On 13 October 2022 23:30 US/Pacific, there was an unexpected increase of incoming and logging traffic combined with a bug in Google\u2019s internal streaming RPC library that triggered a deadlock and cause...",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident appears to be a critical bug in the Streaming RPC Library that caused a deadlock due to high logging traffic, leading to an overload and subsequent error rates in the BigQueryStorageWriteAPI.",
          "score": 0.9
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a critical bug in the StreamingRPC Library that caused a deadlock due to high logging traffic, leading to an overload and subsequent failure of the WriteAPIStreamingFrontend.",
          "score": 0.8
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a critical bug in the Streaming RPC Library that caused a deadlock due to high logging traffic, leading to an overload and subsequent failure of the Write API Streaming Frontend.",
          "score": 0.8
        }
      ],
      "avg_score": 0.833,
      "correct": true
    },
    {
      "id": "incident_122",
      "company": "Google",
      "category": "Infrastructure",
      "ground_truth": "A thundering herd problem when service control tasks restarted, overloading the underlying spanner infrastructure",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a sudden and unexpected spike in load on the region us-central-1, likely due to an unforeseen external factor or a previously undetected internal issue that overwhelmed the service-control binaries.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a failure in the service-control binaries, which led to a cascading effect on all regions and resulted in excessive load on region us-central-1.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a configuration or policy update issue that caused an excessive load on the region us-central-1, leading to a prolonged recovery time and subsequent failures in service control binaries across all regions.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_123",
      "company": "GPS/GLONASS",
      "category": "Software",
      "ground_truth": "(?) this didn't fix the issue",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a previously unknown or unaddressed dependency conflict between the software update and the existing system configuration, which caused a cascade of errors and failures in the orbital mechanics calculations.",
          "score": 0.4
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a dependency conflict between the software update and the existing dependencies in the system, which prevented the successful rollback or reversion of the system.",
          "score": 0.9
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a previously unidentified dependency conflict between the software update and the existing system configuration, which caused a cascade of errors during the rollback process.",
          "score": 1.0
        }
      ],
      "avg_score": 0.767,
      "correct": true
    },
    {
      "id": "incident_124",
      "company": "Healthcare.gov",
      "category": "Infrastructure",
      "ground_truth": "A large organizational failure to build a website for United States healthcare",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident appears to be a failure in the SSL certificate validation, which triggered a chain reaction of subsequent errors and failures in the website build and deployment process.",
          "score": 0.4
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be a failure in the SSL certificate validation, which triggered a chain reaction of subsequent errors and failures throughout the website build and deployment process.",
          "score": 0.4
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a failure in the SSL certificate validation, which triggered a chain reaction of subsequent errors and failures throughout the website build and deployment process.",
          "score": 0.3
        }
      ],
      "avg_score": 0.367,
      "correct": false
    },
    {
      "id": "incident_125",
      "company": "Heroku",
      "category": "Software",
      "ground_truth": "Having a system that requires scheduled manual updates resulted in an error which caused US customers to be unable to scale, stop or restart dynos, or route HTTP traffic, and also prevented all custom...",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident appears to be a database connection timeout issue that prevented the scheduled updates from being applied, leading to a cascade of failures in various services and ultimately causing a critical system failure.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a database connection timeout that prevented the scheduled updates from being applied, leading to a cascade of failures in various services.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a database connection timeout that prevented the scheduled updates from applying, leading to a cascade of failures in various services and ultimately causing a critical system failure.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_126",
      "company": "Heroku",
      "category": "Infrastructure",
      "ground_truth": "An upgrade silently disabled a check that was meant to prevent filesystem corruption in running containers",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is that a recent upgrade to Heroku's system may have inadvertently disabled a critical filesystem check, leading to container corruption and subsequent application failures.",
          "score": 0.9
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is that a recent upgrade to Heroku's system may have inadvertently disabled a critical filesystem check, leading to container corruption and subsequent application failures.",
          "score": 0.9
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is that a recent upgrade to the Heroku platform may have inadvertently disabled a critical filesystem check, which was later identified in a blog post from 2017.",
          "score": 0.0
        }
      ],
      "avg_score": 0.6,
      "correct": false
    },
    {
      "id": "incident_127",
      "company": "Heroku",
      "category": "Infrastructure",
      "ground_truth": "An upstream `apt` update broke pinned packages which lead to customers experiencing write permission failures to `/dev`",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is that an upstream update to the apt package manager broke pinned packages, causing a cascade of failures due to write permission issues with /dev devices.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is that an upstream update to the apt package manager broke pinned packages, causing a cascade of failures across multiple services due to insufficient write permissions on certain devices.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is that an upstream update to the apt package manager broke pinned packages, leading to a cascade of issues with write permissions and access to critical system resources.",
          "score": 0.9
        }
      ],
      "avg_score": 0.867,
      "correct": true
    },
    {
      "id": "incident_128",
      "company": "Heroku",
      "category": "Database",
      "ground_truth": "Private tokens were leaked, and allowed attackers to retrieve data, both in internal databases, in private repositories and from customers accounts",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is a security vulnerability in the Heroku authentication system, specifically related to the handling and storage of private tokens.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident is a misconfigured or insecure private token generation process that allowed an unauthorized external IP to access the Heroku repository and database, resulting in a security breach.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is a misconfigured or weakly generated private token that was leaked into the system logs, allowing unauthorized access to sensitive customer account data.",
          "score": 0.8
        }
      ],
      "avg_score": 0.733,
      "correct": true
    },
    {
      "id": "incident_129",
      "company": "Heroku",
      "category": "Infrastructure",
      "ground_truth": "Increased workload scheduling, caused the application's work queue to build up",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause is a catastrophic failure of the system's worker pool, resulting from an unspecified but critical issue with the underlying infrastructure or configuration that led to a complete collapse of the system's ability to process requests.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause is that the system was unable to scale its router instances correctly, leading to a persistent shortage of available workers, which in turn caused failures in request handling and database connections.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is a sudden and unexpected loss of available worker instances for the scheduler, which led to a cascade of failures across multiple services.",
          "score": 0.0
        }
      ],
      "avg_score": 0.467,
      "correct": false
    },
    {
      "id": "incident_130",
      "company": "Homebrew",
      "category": "Security",
      "ground_truth": "A GitHub personal access token with recently elevated scopes was leaked from Homebrew\u2019s Jenkins that allowed access to `git push` on several Homebrew repositories",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a compromised GitHub token with elevated scopes that was leaked from the Jenkins server, allowing unauthorized access to repositories.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a leaked GitHub token with elevated scopes that was inadvertently granted to the Jenkins service, allowing unauthorized access to repositories.",
          "score": 0.9
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a compromised GitHub token with elevated scopes that was inadvertently granted to the Jenkins service, allowing unauthorized access to repositories.",
          "score": 0.7
        }
      ],
      "avg_score": 0.767,
      "correct": true
    },
    {
      "id": "incident_131",
      "company": "Honeycomb",
      "category": "Infrastructure",
      "ground_truth": "Fast growth",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is likely a database connection issue, possibly due to a configuration problem or a hardware failure, which led to a cascade of failures in the system's request handling loop, resulting in data corruption and ultimately causing the system crash.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is likely a database connection issue, possibly due to a configuration problem or an unhandled write request that led to data corruption and subsequent system crashes.",
          "score": 0.4
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is likely a database connection issue, possibly due to a configuration problem or a hardware failure, that led to a cascade of errors and ultimately caused the system crash.",
          "score": 0.0
        }
      ],
      "avg_score": 0.133,
      "correct": false
    },
    {
      "id": "incident_132",
      "company": "Honeycomb",
      "category": "Infrastructure",
      "ground_truth": "Another story of multiple incidents that ended up impacting [query performance](https://status",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a database connection failure to backend services, which caused high latency on queries and triggered multiple alerts, including SLO breaches.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a failure in the database connection to backend services, which caused high latency on queries, triggered SLO breaches, and impacted alerting via triggers.",
          "score": 0.8
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a database connection issue to backend services, which caused high latency on queries, triggered SLO breaches, and impacted alerting via triggers.",
          "score": 0.8
        }
      ],
      "avg_score": 0.767,
      "correct": true
    },
    {
      "id": "incident_133",
      "company": "Honeycomb",
      "category": "Infrastructure",
      "ground_truth": "On September 8th, 2022, our ingest system went down repeatedly and caused interruptions for over eight hours",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a memory leak in the Service-ingest component that led to high CPU utilization, database connection failures, and ultimately caused the system to crash after an escalation was triggered.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a memory leak in the Service-ingest component that led to high CPU utilization and ultimately caused the system to crash after repeated failures to connect to the database.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a memory leak in the service-ingest that led to high CPU utilization, database connection failures, and ultimately caused the system to crash after an escalation triggered by repeated failed ingestion attempts.",
          "score": 0.0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_134",
      "company": "Honeycomb",
      "category": "Infrastructure",
      "ground_truth": "On July 25th, 2023, we experienced a total Honeycomb outage",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident appears to be a failure in the database connection attempt by Service-f, which triggered a cascade of failures across multiple services and ultimately led to a total system outage.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be a failure in the database connection attempt by Service-f, which led to a cascading failure of multiple services and ultimately resulted in a total system outage.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a failure in the database connection attempt by Service-f, which triggered a chain reaction of failures across multiple services and ultimately led to a total system outage.",
          "score": 0.0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_135",
      "company": "incident.io",
      "category": "Infrastructure",
      "ground_truth": "A bad event (poison pill) in the async workers queue triggered unhandled panics that repeatedly crashed the app",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a \"bad event\" or \"poison pill\" introduced into the async worker queue, which caused a chain reaction of unhandled panics and crashes that ultimately led to the incident.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a \"bad event\" or \"poison pill\" introduced into the async worker queue, which caused a chain reaction of unhandled panics and crashes in multiple services.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a \"bad event\" or \"poison pill\" introduced into the async worker queue, which caused a chain reaction of unhandled panics and crashes in multiple services.",
          "score": 0.9
        }
      ],
      "avg_score": 0.967,
      "correct": true
    },
    {
      "id": "incident_136",
      "company": "Indian Electricity Grid",
      "category": "Infrastructure",
      "ground_truth": "The absence of islanding mechanisms",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident appears to be a failure to enable the islanding mechanism on both the western and eastern grids, leading to repeated under-frequency protection triggers and subsequent circuit breaker trips that caused the grids to collapse.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be a failure to enable the islanding mechanism on both the western and eastern grids, leading to repeated under-frequency protection triggers and subsequent circuit breaker trips that caused the grids to collapse.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a failure to enable the islanding mechanism on both the western and eastern grids, which allowed for uncontrolled power flow between them, leading to under-frequency protection triggers and subsequent circuit breaker trips.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_137",
      "company": "Instapaper",
      "category": "Database",
      "ground_truth": "Also [this](https://web",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is that the system failed to properly authenticate users with the new database, likely due to a misconfiguration or incorrect setup of the authentication mechanism.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident is that the system failed to properly authenticate with the new database, likely due to incorrect or missing credentials being set during the migration process.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is that the maintenance window for database migration was initiated without sufficient disk space being allocated or reserved, leading to a series of subsequent errors and failures during the migration process.",
          "score": 1.0
        }
      ],
      "avg_score": 0.9,
      "correct": true
    },
    {
      "id": "incident_138",
      "company": "Intel",
      "category": "Software",
      "ground_truth": "A scripting bug caused the generation of the divider logic in the Pentium to very occasionally produce incorrect results",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a scripting bug in the division logic calculation, which has led to intermittent and unpredictable results that triggered error handling and ultimately halted the CPU-intensive task.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a scripting bug in the division logic calculation, which has been identified by preliminary diagnosis and further investigation.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a scripting bug in the division logic calculation, which has been identified through preliminary diagnosis by the Intel Monitoring System.",
          "score": 0.7
        }
      ],
      "avg_score": 0.833,
      "correct": true
    },
    {
      "id": "incident_139",
      "company": "Joyent",
      "category": "Database",
      "ground_truth": "A combination of postgresql's transaction wraparound maintenance taking a lock on something, and a joyent query that unnecessarily tried to take a global lock",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a misconfigured or inadequate timeout period for acquiring the global lock on the metadata table, causing repeated attempts to acquire the lock and subsequent timeouts.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a misconfigured or faulty locking mechanism in the PostgreSQL metadata maintenance process that prevents the joyent_query_service from acquiring a global lock on the metadata table, resulting in repeated timeouts and blocked operations.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a misconfigured or faulty locking mechanism in the PostgreSQL metadata maintenance process, which is preventing the joyent_query_service from acquiring a global lock on the metadata table, resulting in repeated timeouts and blocked operations.",
          "score": 0.8
        }
      ],
      "avg_score": 0.733,
      "correct": true
    },
    {
      "id": "incident_140",
      "company": "Joyent",
      "category": "Infrastructure",
      "ground_truth": "An operator used a tool with lax input validation to reboot a small number of servers undergoing maintenance but forgot to type `-n` and instead rebooted all servers in the datacenter",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a lack of proper input validation for the reboot command, which allowed an unauthorized operator to disable the input validation feature, leading to a cascade of errors and system failures.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a lack of proper input validation on the reboot command, which allowed an unauthorized operator to disable input validation and inadvertently caused a cascade of failures across multiple servers.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a lack of proper input validation on the reboot command, which allowed an unauthorized operator to disable the input validation feature, ultimately leading to a cascade of errors and system failures.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_141",
      "company": "Kickstarter",
      "category": "Database",
      "ground_truth": "A mysql bug which sometimes caused `order by` to be ignored",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is likely a misconfigured or failed replication process, which led to an inconsistent primary database state, causing subsequent errors and timeouts.",
          "score": 0.4
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a misconfigured or failing MySQL replication setup, which led to an inconsistent primary database state, causing data replication failures, backup issues, and ultimately resulting in a critical database connection timeout.",
          "score": 0.4
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a misconfigured or failing MySQL replication setup, which led to an inconsistent primary database and subsequent failures in data replication and backup processes.",
          "score": 0.4
        }
      ],
      "avg_score": 0.4,
      "correct": false
    },
    {
      "id": "incident_142",
      "company": "Kings College London",
      "category": "Infrastructure",
      "ground_truth": "3PAR suffered catastrophic outage which highlighted a failure in internal process",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is that the storage team failed to implement a fix for the disk errors that caused the backup failure, resulting in a catastrophic system failure and prolonged downtime.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident is that the storage team failed to implement a fix for the disk errors that caused the backup failure, resulting in a catastrophic system failure and prolonged downtime.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause is that the storage team failed to implement a fix for the disk errors that caused the backup failure, resulting in a catastrophic system failure and prolonged downtime.",
          "score": 0.7
        }
      ],
      "avg_score": 0.733,
      "correct": true
    },
    {
      "id": "incident_143",
      "company": "Launchdarkly",
      "category": "Infrastructure",
      "ground_truth": "Rule attribute selector causing flag targeting web interface to crash",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident appears to be a misconfigured or faulty attribute selector in the flag targeting configuration, which caused a cascade of errors and ultimately led to the web interface crashing.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be a misconfigured or invalid attribute selector in the flag targeting configuration, which caused a cascade of failures and ultimately led to a web interface crash.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a misconfigured or invalid attribute selector in the flag targeting configuration, which caused a cascade of failures throughout the system.",
          "score": 0.7
        }
      ],
      "avg_score": 0.9,
      "correct": true
    },
    {
      "id": "incident_144",
      "company": "Mailgun",
      "category": "Database",
      "ground_truth": "Secondary MongoDB servers became overloaded and while troubleshooting accidentally pushed a change that sent all secondary traffic to the primary MongoDB server, overloading it as well and exacerbatin...",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is a disk space issue on the primary MongoDB node, which led to a cascade of failures and errors as the system struggled to handle incoming requests and recover from the failure.",
          "score": 0.4
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident is that the primary MongoDB node was overloaded with too many connections, causing it to become unable to handle incoming requests and eventually leading to a failure.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is that the primary node in the MongoDB replication setup was overloaded, causing a cascade of failures and errors throughout the system.",
          "score": 0.7
        }
      ],
      "avg_score": 0.6,
      "correct": false
    },
    {
      "id": "incident_145",
      "company": "Mandrill",
      "category": "Database",
      "ground_truth": "Transaction ID wraparound in Postgres caused a partial outage lasting a day and a half",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a Postgres transaction ID wraparound, which occurred when the system's ability to generate unique transaction IDs was overwhelmed, causing duplicate and invalid transaction IDs to be generated, leading to data corruption and partial outage.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a Postgres database transaction ID wraparound, which occurred when the system's ability to generate unique transaction IDs became exhausted, causing subsequent transactions to overwrite existing data and leading to partial data loss and system instability.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident was a Postgres transaction ID wraparound, which occurred when the system's ability to generate unique transaction IDs was exhausted, causing subsequent transactions to overwrite existing ones and leading to data loss, errors, and eventual outage.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_146",
      "company": "Medium",
      "category": "Infrastructure",
      "ground_truth": "Polish users were unable to use their \"\u015a\" key on Medium",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is that an outdated dependency in the system's patching mechanism prevented the application of a necessary patch to resolve the issue with the \"\u015a\" key mapping for Polish keyboard layouts.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is an outdated dependency in the system's patching mechanism, which prevented the application of a necessary patch to resolve the issue with the \"\u015a\" key mapping for Polish keyboard layouts.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is that an outdated dependency in the system's patching process prevented the application of a necessary fix for the \"\u015a\" key issue, leading to a service unavailability for Polish users.",
          "score": 0.8
        }
      ],
      "avg_score": 0.733,
      "correct": true
    },
    {
      "id": "incident_147",
      "company": "Metrist",
      "category": "Cloud",
      "ground_truth": "Azure published a breaking change that affected downstream systems like Metrist's service without warning them, the post covers how to identify the issue and how to recover from it",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is a recent breaking change in Azure that affected the database connection and subsequently impacted downstream services, specifically Service-a and Service-b.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a recent breaking change in Azure that affected multiple downstream services, including Metrist's service, causing a cascade of errors and failures.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a recent breaking change in Azure that affected multiple downstream services, including Service-a and Service-b, causing a cascade of errors and failures.",
          "score": 0.7
        }
      ],
      "avg_score": 0.8,
      "correct": true
    },
    {
      "id": "incident_148",
      "company": "NASA",
      "category": "Infrastructure",
      "ground_truth": "A design flaw in the Apollo 11 rendezvous radar produced excess CPU load, causing the spacecraft computer to restart during lunar landing",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a critical failure in the spacecraft computer's cooling system, which led to repeated CPU overloads and subsequent reboots, ultimately resulting in a launch abort sequence.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is likely a hardware or software failure in the spacecraft's CPU, specifically a malfunctioning cooling system that caused excessive heat buildup and subsequent crashes during repeated reboots.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is likely a hardware or software failure in the spacecraft's computer system, specifically a critical component such as a processor or memory module, that caused repeated CPU overloads and subsequent reboots, ultimately leading to a launch abort sequence.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_149",
      "company": "NASA",
      "category": "Software",
      "ground_truth": "Use of different units of measurement (metric vs",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a mismatch between metric and English units in the navigation data, which led to a cascade of subsequent errors and system alerts.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a mismatch between metric and English units in the navigation data, which led to a cascade of subsequent errors and system failures.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a mismatch between metric and English units in the navigation data, which led to a cascade of subsequent errors and system alerts.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_150",
      "company": "NASA",
      "category": "Software",
      "ground_truth": "NASA's Mars Pathfinder spacecraft experienced system resets a few days after landing on Mars (1997)",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a critical failure in the VxWorks task scheduler's ability to properly manage priority levels, leading to repeated and escalating system resets due to insufficient disk space for patch updates.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a critical failure in the VxWorks task scheduler, which led to a cascade of subsequent errors and ultimately resulted in the loss of mission parameters on the Mars Pathfinder spacecraft.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a critical failure in the VxWorks task scheduler, which led to a cascade of subsequent issues and ultimately resulted in the loss of mission parameters on the Mars Pathfinder spacecraft.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_151",
      "company": "Netflix",
      "category": "Infrastructure",
      "ground_truth": "An EBS outage in one availability zone was mitigated by migrating to other availability zones",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is likely a misconfigured or failing EBS volume in AZ1, which caused the instance to become unavailable and required manual intervention for data migration to other Availability Zones (AZs) to restore service.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is likely a misconfigured or failing EBS volume in AZ1, which caused a request failure with error code 503 (Service Unavailable), triggering an alert and subsequent migration of data to other Availability Zones.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a failure in the EBS volume, which led to a request failing with an error code 503 (Service Unavailable), resulting in a potential outage and subsequent migration of data to other Availability Zones (AZs).",
          "score": 0.0
        }
      ],
      "avg_score": 0.667,
      "correct": false
    },
    {
      "id": "incident_152",
      "company": "North American Electric Power System",
      "category": "Infrastructure",
      "ground_truth": "A power outage in Ohio around 1600h EDT cascaded up through a web of systemic vulnerabilities and process failures and resulted in an outage in the power grid affecting ~50,000,000 people for ~4 days ...",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this power grid stability incident was likely a combination of human error, inadequate testing, or insufficient maintenance leading to a cascading failure in the grid management system's scheduling and monitoring processes.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of this power grid stability incident was likely an unexpected software bug or configuration issue in the Grid Management System, which triggered a cascade of failures leading to widespread power outages across multiple regions.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of this power grid stability issue was likely a combination of inadequate maintenance planning, insufficient testing of regional schedules, and unforeseen interactions between different system components, ultimately leading to a cascading failure that resulted in widespread power outages across multiple regions.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_153",
      "company": "Okta",
      "category": "Infrastructure",
      "ground_truth": "A hackers group got access to a third-party support engineer's laptop",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is that a third-party support engineer's laptop was compromised, allowing a hacker group to gain unauthorized access and exfiltrate sensitive data.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident is likely a phishing or social engineering attack that tricked the support engineer into installing malware on their laptop, which then allowed unauthorized access to the Okta authentication service.",
          "score": 0.8
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is that a third-party support engineer's laptop, which had access to sensitive Okta systems, was compromised by a hacker group through an unknown vulnerability.",
          "score": 0.8
        }
      ],
      "avg_score": 0.767,
      "correct": true
    },
    {
      "id": "incident_154",
      "company": "OpenAI",
      "category": "Database",
      "ground_truth": "Queues for requests and responses in a Redis cache became corrupted and out of sequence, leading to some requests revealing other people's user data to some users, including app activity data and some...",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is likely a misconfigured or poorly managed Redis instance that led to a cascade of errors and security breaches due to the accumulation of corrupted cache data.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a catastrophic failure of the Redis cache system, likely due to a combination of factors including insufficient disk space, inadequate configuration, and/or hardware failure, which led to a cascade of errors and security breaches.",
          "score": 0.6
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is that the Redis cache became corrupted due to a combination of factors, including an excessive request queue length, failed flushing operations, and subsequent attempts to recover from cache corruption, ultimately leading to a cascade of errors and security breaches.",
          "score": 0.8
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_155",
      "company": "Pagerduty",
      "category": "Network",
      "ground_truth": "To high network latency and hence failed in their ability to send notifications",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is likely a network congestion or peering point instability issue that caused high latency on both incoming and outgoing traffic, ultimately leading to failed notification attempts and service failures.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a peering point instability that resulted in high latency across all deployments, causing service failure and notification attempts to fail.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a peering point instability that resulted in high latency across all deployments, causing service failure and subsequent notification attempts to fail.",
          "score": 0.8
        }
      ],
      "avg_score": 0.767,
      "correct": true
    },
    {
      "id": "incident_156",
      "company": "PagerDuty",
      "category": "Cloud",
      "ground_truth": "Aws having issues in a region",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was an unannounced and unexpected AWS outage in the us-west-2 region, which caused a cascade of failures across multiple services reliant on that region.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was an AWS outage in the us-west-2 region, which prevented the PagerDuty monitoring script from resolving the region and subsequently caused a cascade of errors affecting multiple services.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is a scheduled maintenance window for AWS that unexpectedly caused an outage in the US-West-2 region, which subsequently triggered a chain reaction of alerts and notifications across multiple services.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_157",
      "company": "Parity",
      "category": "Security",
      "ground_truth": "$30 million of cryptocurrency value was diverted (stolen) with another $150 million diverted to a safe place (rescued), after a 4000-line software change containing a security bug was mistakenly label...",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the Parity IT incident was a critical security bug in the software patch that allowed for unauthorized access and diversion of $30 million worth of cryptocurrency.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the Parity IT incident appears to be a critical vulnerability in the software patch deployment process, specifically a bug that allowed for unauthorized access and diversion of sensitive funds.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the Parity IT incident was a critical vulnerability in the software patch deployment process, specifically related to inadequate review and testing of the UI change, which allowed unauthorized access to sensitive data.",
          "score": 0.0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_158",
      "company": "Platform.sh",
      "category": "Infrastructure",
      "ground_truth": "Outage during a scheduled maintenance window because there were too much data for Zookeeper to boot",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was an insufficient data size reported by ZooKeeper due to disk space constraints.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was an insufficient data size reported by ZooKeeper due to disk space constraints.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a lack of sufficient data size in ZooKeeper, which led to an insufficient disk space report and ultimately caused the system services to become unavailable.",
          "score": 0.3
        }
      ],
      "avg_score": 0.567,
      "correct": false
    },
    {
      "id": "incident_159",
      "company": "Reddit",
      "category": "Infrastructure",
      "ground_truth": "An error during a migration of a critical backend system",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was an incorrect database configuration setting that led to a cascade of failures during the backend system migration.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was an incorrect database configuration setting that led to a cascade of failures during the backend system migration.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was an incorrect database configuration setting that led to a cascade of failures during the backend system migration.",
          "score": 0.7
        }
      ],
      "avg_score": 0.8,
      "correct": true
    },
    {
      "id": "incident_160",
      "company": "Reddit",
      "category": "Network",
      "ground_truth": "Node metadata that changed between versions which brought down workload networking",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident appears to be a failure in the etcd node metadata update process, which led to a cascade of failures and ultimately resulted in the Kubernetes cluster becoming unavailable.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be a failure in the etcd node metadata update process, which led to a cascade of failures across various components, ultimately resulting in the Kubernetes cluster becoming unavailable and taking several hours to recover.",
          "score": 0.8
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a failure in the etcd node metadata update process, which led to a cascade of failures and ultimately resulted in the Kubernetes cluster becoming unavailable.",
          "score": 0.7
        }
      ],
      "avg_score": 0.767,
      "correct": true
    },
    {
      "id": "incident_161",
      "company": "Roblox",
      "category": "Database",
      "ground_truth": "Roblox end Oct 2021 73 hours outage",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a Consul cluster connection issue that led to a cascade of failures in multiple services, ultimately resulting in data loss and system failure.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a Consul cluster connection issue that led to a cascade of failures across multiple services, ultimately resulting in data loss and system failure due to inconsistent state detection.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a Consul cluster connection issue that led to a cascade of failures across multiple services, ultimately resulting in data loss and system failure.",
          "score": 0.0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_162",
      "company": "Salesforce",
      "category": "Database",
      "ground_truth": "Power failure in one datacenter led to cascading failures with a database cluster and file discrepancies resulting in cross data center failover issues",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is a failure in the datacenter's power infrastructure, specifically a single point of failure that caused a cascade of failures across multiple datacenters and systems.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a power failure in one datacenter (DC1) that caused a cascade of failures across multiple systems and datacenters, including database clusters, file inconsistencies, and system failures.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a power failure in one datacenter (DC1) that triggered a chain reaction of events, including file discrepancies and database cluster failures, due to inadequate backup and failover procedures.",
          "score": 1.0
        }
      ],
      "avg_score": 0.933,
      "correct": true
    },
    {
      "id": "incident_163",
      "company": "Salesforce",
      "category": "Security",
      "ground_truth": "On September 20, 2023, a service disruption affected a subset of customers across multiple services beginning at 14:48 Coordinated Universal Time (UTC)",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is an unexpected policy change that was scheduled for review and update, which inadvertently caused a cascade of authentication failures across multiple services, ultimately leading to widespread denial of access to customer resources.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was an unexpected policy change that triggered a cascade of errors and security issues, ultimately leading to widespread denial of access for customers in affected services.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is an unexpected policy change that was scheduled for review and update, which inadvertently triggered a cascade of errors and security controls that ultimately led to widespread denial of access to customer resources.",
          "score": 0.0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_164",
      "company": "Sentry",
      "category": "Database",
      "ground_truth": "Transaction ID Wraparound in Postgres caused Sentry to go down for most of a working day",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause is likely a misconfigured or inadequate database connection pool, leading to an unsustainable high transaction volume that caused a transaction ID wraparound, resulting in the loss of the database connection and subsequent unavailability of Sentry.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident is likely a misconfiguration or insufficient tuning of the PostgreSQL database's transaction ID wraparound mechanism, which allowed a large batch of queries from service-a to trigger a transaction ID wraparound, causing the database connection to become unavailable.",
          "score": 0.9
        },
        {
          "run": 3,
          "prediction": "The root cause is likely that the high transaction volume detected in the database led to a transaction ID wraparound, causing a cascade of errors and ultimately resulting in the loss of the database connection.",
          "score": 0.7
        }
      ],
      "avg_score": 0.767,
      "correct": true
    },
    {
      "id": "incident_165",
      "company": "Shapeshift",
      "category": "Security",
      "ground_truth": "Poor security practices enabled an employee to steal $200,000 in cryptocurrency in 3 separate hacks over a 1 month period",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this series of incidents is a compromised encryption key, which allowed unauthorized users to access sensitive data and transfer funds, ultimately resulting in a significant financial loss.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the series of incidents is a compromised encryption key that allowed unauthorized access to sensitive data, leading to multiple instances of unauthorized access, theft, and financial loss.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incidents is a compromised encryption key used across multiple services, allowing unauthorized access to sensitive data and resulting in financial losses.",
          "score": 0.0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_166",
      "company": "Skyliner",
      "category": "Memory",
      "ground_truth": "A memory leak in a third party library lead to Skyliner being unavailable on two occasions",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a memory leak in the third-party library \"lib-XYZ.so\" that was loaded by service-e, causing a gradual increase in memory usage leading to repeated out-of-memory errors and eventual unavailability of the Skyliner system.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a memory leak in the third-party library \"lib-XYZ.so\" that was loaded by service-e, which continuously increased memory usage without being properly released.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a memory leak in the third-party library \"lib-XYZ.so\" that was not properly handled, leading to continuous increase in memory usage and eventual unavailability of the Skyliner service.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_167",
      "company": "Slack",
      "category": "Database",
      "ground_truth": "A combination of factor results in a large number of Slack's users being disconnected to the server",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident appears to be a failure in the database connection management system, likely due to inadequate resource allocation or insufficient capacity planning, which led to a cascade of disconnections and authentication failures.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a failure in the database connection management system, which led to an exponential increase in disconnections and subsequent cascading failures, ultimately resulting in a prolonged outage that affected 5% of users.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a failure in the database connection management system, specifically a lack of sufficient capacity and/or inadequate error handling mechanisms that allowed the disconnection attempts to exceed the capacity limit, leading to a cascade of failures.",
          "score": 0.7
        }
      ],
      "avg_score": 0.8,
      "correct": true
    },
    {
      "id": "incident_168",
      "company": "Slack",
      "category": "Network",
      "ground_truth": "Network saturation in AWS's traffic gateways caused packet loss",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a sudden and unexpected surge in traffic volume that overwhelmed the system's request handling capacity, leading to packet loss and ultimately causing a cascade of failures across multiple services.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a sudden and unexpected surge in traffic volume that overwhelmed the system's request handling capabilities, leading to packet loss, service outages, and ultimately, a system failure.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a sudden and unexpected surge in traffic volume that overwhelmed the system's capacity, leading to packet loss and subsequent failures across multiple services.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_169",
      "company": "Slack",
      "category": "Infrastructure",
      "ground_truth": "Cache nodes removal caused the high workload on the vitness cluster, which in turn cased the service outage",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was the premature removal of cache nodes from the witness cluster, which overloaded the remaining nodes and caused a cascade of service failures.",
          "score": 0.9
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was the premature removal of cache nodes from the witness cluster during maintenance, which led to a high workload on the remaining nodes and subsequently caused service outages.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was the premature removal of cache nodes from the witness cluster, which overloaded the remaining nodes and caused a cascade of failures across multiple services.",
          "score": 0.9
        }
      ],
      "avg_score": 0.833,
      "correct": true
    },
    {
      "id": "incident_170",
      "company": "Spotify",
      "category": "Infrastructure",
      "ground_truth": "Lack of exponential backoff in a microservice caused a cascading failure, leading to notable service degradation",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a high CPU usage issue in service-b, which led to a cascade of failures and ultimately caused all services to go down due to insufficient backoff mechanisms and database connection timeouts.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a high CPU usage issue in service-b, which led to a cascade of failures across multiple services, including database connection timeouts, failed user credential validation, and ultimately, a complete system unavailability.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a high CPU usage issue in service-b, which led to a cascade of failures across multiple services, including database connection timeouts, failed user credential validation, and ultimately, a complete system unavailability.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_171",
      "company": "Square",
      "category": "Security",
      "ground_truth": "A cascading error from an adjacent service lead to merchant authentication service being overloaded",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was an unexpected and unanticipated surge in requests from a specific adjacent service (service-b) that overwhelmed the merchant-authentication-service, causing a cascade of failures and impacting user experience.",
          "score": 0.9
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was an unexpected and unanticipated increase in requests from a specific adjacent service (service-b) that overwhelmed the merchant-authentication-service, causing a cascade of subsequent failures.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was an unexpected and unanticipated increase in requests from a specific adjacent service (service-b) that overwhelmed the merchant-authentication-service, causing a cascade of failures and ultimately requiring manual intervention to recover.",
          "score": 1.0
        }
      ],
      "avg_score": 0.967,
      "correct": true
    },
    {
      "id": "incident_172",
      "company": "Stackdriver",
      "category": "Database",
      "ground_truth": "In October 2013, [Stackdriver](https://www",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a misconfigured replication factor, which led to a cascade of errors and ultimately caused the Cassandra cluster to go down.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a misconfigured replication factor in the Cassandra cluster, which led to connection errors and ultimately resulted in the cluster going down.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a misconfigured replication factor in the Cassandra cluster, which led to connection errors and ultimately caused the cluster to go down.",
          "score": 0.0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_173",
      "company": "Stack Exchange",
      "category": "Infrastructure",
      "ground_truth": "Enabling StackEgg for all users resulted in heavy load on load balancers and consequently, a DDoS",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident appears to be a sudden and unexpected surge in traffic to StackEgg, which overwhelmed the load balancer and triggered a series of cascading failures, including DDoS attacks, hardware failures, and system crashes.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is likely an unexpected and unanticipated surge in traffic to StackEgg, which overwhelmed the load balancer and triggered a chain reaction of failures across multiple services.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is likely an unexpected and unanticipated surge in traffic to StackEgg, which overwhelmed the load balancer and triggered a chain reaction of subsequent failures.",
          "score": 0.7
        }
      ],
      "avg_score": 0.733,
      "correct": true
    },
    {
      "id": "incident_174",
      "company": "Stack Exchange",
      "category": "Infrastructure",
      "ground_truth": "Backtracking implementation in the underlying regex engine turned out to be very expensive for a particular post leading to health-check failures and eventual outage",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was an unoptimized backtracking implementation in the regex engine that caused a timeout and subsequent failures in the health check process for post #1234567890.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was an unoptimized backtracking implementation in the regex engine that caused a cascade of failures across multiple services and ultimately led to the entire Stack Exchange server cluster going down.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was an unoptimized backtracking implementation in the regex engine for post #1234567890, which caused a cascade of failures across multiple services and ultimately led to the entire Stack Exchange server cluster going down.",
          "score": 1.0
        }
      ],
      "avg_score": 0.9,
      "correct": true
    },
    {
      "id": "incident_175",
      "company": "Stack Exchange",
      "category": "Software",
      "ground_truth": "Porting old Careers 2",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a user information leak in the Careers 2.0 migration process, which was not properly handled by the migration-executor service, leading to an exception being thrown during porting and ultimately triggering a security incident notification.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be a misconfigured migration process that resulted in a user information leak during the porting of Careers 2.0, which was not properly handled by the migrations-controller before being alerted to the team.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a user information leak detected during the porting process of Careers 2.0 to Developer Story, which was likely caused by a misconfigured or insecure data transfer mechanism between services.",
          "score": 0.0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_176",
      "company": "Stack Exchange",
      "category": "Database",
      "ground_truth": "The primary SQL-Server triggered a bugcheck on the SQL Server process, causing the Stack Exchange sites to go into read only mode, and eventually a complete outage",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident appears to be a catastrophic failure of the primary SQL Server process due to corruption in the database, which led to a chain reaction of failures across multiple sites and ultimately resulted in permanent data loss.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident is a corruption in the SQL Server database that caused a chain reaction of failures leading to a complete outage and potential permanent data loss.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a corruption in the SQL Server database that led to a cascade of failures and ultimately resulted in permanent data loss.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_177",
      "company": "Strava",
      "category": "Infrastructure",
      "ground_truth": "Hit the signed integer limit on a primary key, causing uploads to fail",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause is that the system's database schema was not designed with a sufficient signed integer limit for its primary key column, leading to an overflow and subsequent failures in data upload.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is an integer overflow in a primary key column, which exceeded the signed integer limit, causing a cascade of errors and failures in subsequent database operations.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause is that the system's database schema and configuration did not account for the limitations of signed integers, leading to a primary key counter exceeding its threshold.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_178",
      "company": "Stripe",
      "category": "Database",
      "ground_truth": "Manual operations are regularly executed on production databases",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident appears to be a misconfiguration or incomplete setup of the database dependencies, specifically missing dependencies between Service-A and Service-B, which led to a cascade of errors and ultimately resulted in the Stripe API being down for an extended period.",
          "score": 0.4
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a misconfigured database dependency, specifically that Service-B was missing a required dependency, which caused a cascade of errors and ultimately led to the Stripe API being down for an extended period.",
          "score": 0.4
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a misconfigured or missing database dependency, which caused a cascade of errors and ultimately led to the Stripe API being down for an extended period.",
          "score": 0.6
        }
      ],
      "avg_score": 0.467,
      "correct": false
    },
    {
      "id": "incident_179",
      "company": "Sweden",
      "category": "Infrastructure",
      "ground_truth": "Use of different rulers by builders caused the _Vasa_ to be more heavily built on its port side and the ship's designer, not having built a ship with two gun decks before, overbuilt the upper decks, l...",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was an uneven construction design in the ship's blueprints, specifically a top-heavy design that exceeded safe limits due to inconsistent ruler usage among builders.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was an uneven construction of the ship's hull due to inconsistent ruler usage among builders, which led to a top-heavy design that exceeded safe limits and ultimately caused the catastrophic sinking.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident was an uneven construction design of the ship's hull, which led to a top-heavy design that exceeded safe limits and put the ship at risk of catastrophic failure.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_180",
      "company": "Tarsnap",
      "category": "Cloud",
      "ground_truth": "A batch job which scans for unused blocks in Amazon S3 and marks them to be freed encountered a condition where all retries for freeing certain blocks would fail",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is that the system's local disk usage exceeded 90%, causing a filesystem warning and ultimately resulting in a filesystem failure, which prevented the Tarsnap service from writing log entries and led to a cascade of failures.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident is that the local disk usage exceeded 90%, causing a filesystem warning and ultimately leading to a critical failure when the log file could not be written, resulting in service shutdown.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is that the local disk usage exceeded 90%, causing a filesystem warning, which ultimately led to a filesystem failure and subsequent service stop due to inability to write log entries.",
          "score": 0.0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_181",
      "company": "Telstra",
      "category": "Infrastructure",
      "ground_truth": "A fire in a datacenter caused SMS text messages to be sent to random destinations",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident appears to be a hardware or environmental issue that caused a fire in the datacenter, which led to a cascade of failures and security breaches.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be a hardware or environmental issue that caused a fire in the datacenter, which led to a cascade of failures and errors in the SMS service, ultimately resulting in messages being sent to random destinations.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident appears to be a hardware or environmental issue, such as a fire in the datacenter, which caused a cascade of failures and security breaches, ultimately leading to the loss of control over SMS messaging.",
          "score": 1.0
        }
      ],
      "avg_score": 0.833,
      "correct": true
    },
    {
      "id": "incident_182",
      "company": "Therac-25",
      "category": "Memory",
      "ground_truth": "The Therac-25 was a radiation therapy machine involved in at least six accidents between 1985 and 1987 in which patients were given massive overdoses of radiation",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the system failure was a design flaw in the concurrent programming model, allowing multiple threads to access shared resources simultaneously without proper synchronization, leading to a cascade of errors and ultimately causing the system to fail.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a failure to implement adequate concurrency control mechanisms in the system's programming, allowing multiple threads to access and modify critical data simultaneously, leading to a cascade of errors.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a failure to implement adequate concurrency control measures in the system's programming, allowing multiple threads to access shared resources simultaneously and causing a cascade of errors.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_183",
      "company": "trivago",
      "category": "Security",
      "ground_truth": "A human error, all engineers lost access to the central source code management platform (github organization)",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a misconfigured Azure AD security group that was removed during task execution, causing all engineers to lose access to the central source code management platform (GitHub organization).",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is likely a misconfigured or incorrect Azure AD security group policy that removed the necessary permissions for the GitHub organization, resulting in all engineers losing access to the central source code management platform.",
          "score": 0.8
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is that a security group was removed during task execution, leading to a cascade of errors and ultimately resulting in all engineers losing access to the central source code management platform.",
          "score": 0.7
        }
      ],
      "avg_score": 0.733,
      "correct": true
    },
    {
      "id": "incident_184",
      "company": "Twilio",
      "category": "Database",
      "ground_truth": "In 2013, a temporary network partition in the redis cluster used for billing operations, caused a massive resynchronization from slaves",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a network partition that caused multiple slave nodes to struggle with syncing, leading to a cascading failure of the Redis cluster and ultimately resulting in the auto-recharge component becoming unresponsive.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident appears to be a network partition that caused the slave nodes in the Redis cluster to struggle with syncing, leading to a cascade of failures across multiple services.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is likely a misconfigured or inadequate load balancing mechanism, which resulted in the master node being overwhelmed with requests from the auto-recharge component and other services, leading to its crash and subsequent cascading failures.",
          "score": 0.4
        }
      ],
      "avg_score": 0.133,
      "correct": false
    },
    {
      "id": "incident_185",
      "company": "Twilio",
      "category": "Network",
      "ground_truth": "Twilio's incident of having high filtering on SMS towards AT&T Network In United States",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a misconfigured or incorrect setting on the Twilio SMS filtering rules, specifically targeting the AT&T network in the US.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a misconfiguration or incorrect setting of the SMS filtering rules, specifically allowing high filtering on the AT&T network in the US.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was that high filtering was detected on the AT&T network in the US, which blocked SMS send failures to this network.",
          "score": 1.0
        }
      ],
      "avg_score": 0.9,
      "correct": true
    },
    {
      "id": "incident_186",
      "company": "Valve",
      "category": "Software",
      "ground_truth": "Steam's desktop client deleted all local files and directories",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is a missing dependency 'libvdpau.so' that was not properly resolved or installed, leading to a chain reaction of errors and ultimately causing the Steam desktop client to terminate unexpectedly.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a missing dependency 'libvdpau.so' which led to a chain reaction of errors and ultimately caused the Steam desktop client to terminate unexpectedly, requiring a reinstall.",
          "score": 0.0
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is a missing dependency 'libvdpau.so' that was not properly resolved or installed, leading to a cascade of errors and ultimately causing the Steam desktop client to terminate unexpectedly.",
          "score": 0.0
        }
      ],
      "avg_score": 0.0,
      "correct": false
    },
    {
      "id": "incident_187",
      "company": "Xubuntu",
      "category": "Infrastructure",
      "ground_truth": "The website for Xubuntu, a derivative of Ubuntu Linux that uses Xfce as its desktop environment, was compromised by an attacker who changed download links to a malicious zip file",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was an outdated PHP version that was exploited by attackers, allowing them to compromise the Xubuntu website and deploy a malicious zip file.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was an exploitation of a known, yet unpatched, vulnerability in the outdated PHP version used by the Xubuntu website.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was an exploitation of a vulnerable PHP version, which allowed malicious actors to inject and execute zip files on the compromised website.",
          "score": 0.7
        }
      ],
      "avg_score": 0.8,
      "correct": true
    },
    {
      "id": "incident_188",
      "company": "Yeller",
      "category": "Network",
      "ground_truth": "Cached routes, but there wasn't enough logging information to tell for sure",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident appears to be a misconfigured or faulty cluster manager that failed to properly manage the delayed start of services, leading to a cascade of delays and ultimately requiring a rolling restart to restore the cluster state.",
          "score": 0.0
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident is a misconfigured or faulty cluster manager that failed to properly manage and synchronize the delayed start times of services A, C, and D, leading to a cascade of errors and delays.",
          "score": 0.1
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident appears to be a misconfigured or faulty cluster management system that failed to properly handle delayed messages and timeouts, leading to a cascade of errors and ultimately requiring a rolling restart of the entire cluster.",
          "score": 0.0
        }
      ],
      "avg_score": 0.033,
      "correct": false
    },
    {
      "id": "incident_189",
      "company": "Zerodha",
      "category": "Infrastructure",
      "ground_truth": "The Order Management System (OMS) provided to Zerodha, a stock broker, collapsed when an order for 1M units of a penny stock was divided into more than 0",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a division by zero error in the trade calculation logic of the order splitting mechanism, which led to an internal logic failure that caused the Order Management System (OMS) to collapse.",
          "score": 1.0
        },
        {
          "run": 2,
          "prediction": "The root cause of the Order Management System collapse is a division by zero error in trade calculation due to an incorrect assumption about the maximum allowed limit for individual trades.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a division by zero error in the trade calculation logic of the OMS service's splitting mechanism for large orders.",
          "score": 1.0
        }
      ],
      "avg_score": 1.0,
      "correct": true
    },
    {
      "id": "incident_190",
      "company": "Zerodha",
      "category": "Infrastructure",
      "ground_truth": "A failure of the primary leased line to a CTCL between a stock broker and a stock exchange led to the activation of a backup leased line that was operating sporadically over the following hour, affect...",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a failure in the leased line connection to CTCL, which led to a cascade of failures in trading operations and delayed order placement and validation.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a failure in the leased line connection to CTCL, which led to a cascade of failures in trading operations due to the reliance on backup lines and subsequent issues with order validation and processing.",
          "score": 0.8
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a failure in the leased line connection with the provider (CTCL), which resulted in intermittent and unreliable connectivity, causing multiple trading operations failures and delays.",
          "score": 0.7
        }
      ],
      "avg_score": 0.733,
      "correct": true
    },
    {
      "id": "incident_191",
      "company": "Roblox",
      "category": "Infrastructure",
      "ground_truth": "Hashicorp consul cluster failure when enabling a new streaming feature under high load",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is that the Consul cluster was not properly initialized before enabling a new streaming feature under high load, leading to a cascade of failures and outages across multiple services.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is that the new streaming feature was enabled under high load without waiting for the Consul cluster to become available, resulting in a cascade of failures and outages.",
          "score": 0.9
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is likely a misconfiguration or inadequate testing of the new streaming feature under high load, leading to an unavailability of the Consul cluster and subsequent cascading failures across multiple services.",
          "score": 0.7
        }
      ],
      "avg_score": 0.767,
      "correct": true
    },
    {
      "id": "incident_192",
      "company": "Fastly",
      "category": "Software",
      "ground_truth": "Software bug triggered by valid customer configuration change",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the global CDN outage was a misinterpreted configuration update that led to incorrect assumptions about the impact of the changes on the services.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the global CDN outage was a misinterpreted configuration update that led to incorrect assumptions about the impact of changes on affected services.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the global CDN outage was a misinterpreted configuration update that led to incorrect assumptions about the impact of the changes on the affected services.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_193",
      "company": "GitLab",
      "category": "Database",
      "ground_truth": "Accidental deletion of production data during maintenance, with backup failures compounding recovery",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a failure in the database backup process, specifically due to a locked database file issue that prevented the backup script from completing successfully.",
          "score": 0.4
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is that a critical database backup process failed due to an unhandled locking issue, resulting in production data loss and subsequent system failures.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a failure in the database backup process, specifically a locked database file issue that prevented the backup script from running successfully and ultimately led to the loss of production data.",
          "score": 0.7
        }
      ],
      "avg_score": 0.6,
      "correct": false
    },
    {
      "id": "incident_194",
      "company": "Spotify",
      "category": "Software",
      "ground_truth": "Cascading failures in microservices when circuit breakers failed to activate properly",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a database connection timeout in service B, which triggered a cascading failure across multiple services due to the inability of circuit breaker D to activate and prevent further requests from being sent.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a failure in the circuit breaker mechanism, which failed to detect and prevent cascading failures due to an underlying issue with the database connection timeout in service B.",
          "score": 0.8
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a database connection timeout in service B, which triggered a cascade failure across multiple services due to the circuit breaker's inability to activate and detect failures effectively.",
          "score": 0.7
        }
      ],
      "avg_score": 0.767,
      "correct": true
    },
    {
      "id": "incident_195",
      "company": "LinkedIn",
      "category": "Configuration",
      "ground_truth": "Confusion between restart and reload commands affecting production traffic routing",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a misconfigured reload command for service-b, which caused an invalid routing configuration to be sent to service-a, leading to a cascade of failures and inconsistencies across multiple services.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a misconfigured reload command for service-b that caused an invalid routing configuration to be sent to service-a, leading to a cascade of failures and inconsistencies across multiple services.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident was a misconfigured reload command for service-b, which caused an invalid routing configuration to be sent to service-a, leading to a cascade of errors and ultimately requiring a manual intervention by the ops team.",
          "score": 0.7
        }
      ],
      "avg_score": 0.7,
      "correct": true
    },
    {
      "id": "incident_196",
      "company": "Dropbox",
      "category": "Infrastructure",
      "ground_truth": "Disk hot spots and uneven data distribution across shards",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a hardware failure or design flaw that resulted in one shard experiencing excessive heat (85\u00b0C) due to an undetected disk hot spot, leading to a cascade of errors and eventual resolution through emergency manual cooling measures.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident was a failure to properly monitor and manage the temperature of one or more disks, leading to disk hot spots that caused rebalancing issues.",
          "score": 0.7
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a hardware failure resulting in a disk hot spot, which caused an imbalance in data distribution across shards, leading to a cascade of failures that required emergency rebalancing measures.",
          "score": 0.8
        }
      ],
      "avg_score": 0.767,
      "correct": true
    },
    {
      "id": "incident_197",
      "company": "Netflix",
      "category": "Infrastructure",
      "ground_truth": "Regional availability issues exposed by Chaos Monkey testing, revealing hidden dependencies in microservice mesh",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is a complex interplay of factors, but the underlying root cause appears to be a misconfigured or inadequate microservice mesh that created hidden dependencies and circular references between services, leading to a cascade of failures and unavailability across multiple services.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a complex interplay of factors, but if I had to identify one sentence that summarizes the root cause, it would be:\n\n\"The root cause was a failure in the microservice mesh's dependency resolution mechanism, which led to a cascade of failures across multiple services.\"\n\nThis suggests that there was an issue with how the microservice mesh managed dependencies between services, leading to a ripple effect of failures and unavailability of critical services.",
          "score": 0.8
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a failure in the microservice mesh's dependency resolution mechanism, which resulted in unresolvable regional availability issues for multiple services.",
          "score": 0.8
        }
      ],
      "avg_score": 0.8,
      "correct": true
    },
    {
      "id": "incident_198",
      "company": "Uber",
      "category": "Database",
      "ground_truth": "Mysql table growth exceeding operational limits, requiring emergency sharding",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident was a failure to properly configure or maintain the MySQL database's partitioning mechanism, leading to a cascade of errors and failures that ultimately resulted in an unexpected sharding failure.",
          "score": 0.7
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is that the table 'uber_data' exceeded its operational limit (1000 MB) and triggered an emergency sharding protocol, which failed due to a connection timeout on shard 1, leading to a cascading failure of the sharding service.",
          "score": 1.0
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is that the system was unable to successfully partition the 'uber_data' table before it reached its operational limit, leading to a cascade of failures and ultimately requiring manual intervention to recover.",
          "score": 0.8
        }
      ],
      "avg_score": 0.833,
      "correct": true
    },
    {
      "id": "incident_199",
      "company": "Pinterest",
      "category": "Infrastructure",
      "ground_truth": "Message queue overflow during traffic spike, leading to delayed alerts",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of the incident is a design flaw or misconfiguration in the message-queue system that allowed it to consume excessive resources and overflow its maximum capacity, leading to a cascade of failures across multiple services.",
          "score": 0.8
        },
        {
          "run": 2,
          "prediction": "The root cause of the incident is a design flaw or inadequate configuration in the message-queue system that allowed it to consume excessive space and overflow its maximum capacity, leading to a cascade of failures across multiple services.",
          "score": 0.8
        },
        {
          "run": 3,
          "prediction": "The root cause of the incident is a design flaw or inadequate configuration in the message-queue system that allowed it to consume excessive resources and overflow its capacity, leading to a cascade of failures across multiple services.",
          "score": 0.7
        }
      ],
      "avg_score": 0.767,
      "correct": true
    },
    {
      "id": "incident_200",
      "company": "Airbnb",
      "category": "Software",
      "ground_truth": "Network partitions and idempotency key failures",
      "runs": [
        {
          "run": 1,
          "prediction": "The root cause of this incident is a persistent network partition that prevented the idempotency key generation and update, leading to duplicate charge attempts and subsequent critical issues with the payment system.",
          "score": 0.9
        },
        {
          "run": 2,
          "prediction": "The root cause of this incident is a persistent network partition that prevented the idempotency key generation and update, leading to duplicate charge attempts and subsequent critical issues with the payment system.",
          "score": 0.9
        },
        {
          "run": 3,
          "prediction": "The root cause of this incident is a persistent network partition that prevented the idempotency key generation and update, leading to duplicate charge attempts and subsequent critical issues with the payment system.",
          "score": 0.9
        }
      ],
      "avg_score": 0.9,
      "correct": true
    }
  ]
}