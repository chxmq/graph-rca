#!/usr/bin/env python3
"""
Parser Accuracy Experiment
Tests LLM parsing accuracy on LogHub datasets (BGL, HDFS).
CORRECTED: Uses app.utils.log_parser.LogParser to test actual application logic.
"""

import os
import sys
import json
import time
import statistics
import urllib.request
from pathlib import Path
from datetime import datetime

# --- Backend Integration ---
sys.path.insert(0, str(Path(__file__).resolve().parents[2] / "backend"))
try:
    from app.utils.log_parser import LogParser
    from app.models.parsing_data_models import LogEntry
except ImportError as e:
    print(f"Error importing backend modules: {e}")
    sys.exit(1)
# ---------------------------

if 'SSL_CERT_FILE' in os.environ:
    del os.environ['SSL_CERT_FILE']

import ollama

CONFIG = {
    "ollama_host": "http://localhost:11434",
    "model": "llama3.2:3b",
    "runs_per_log": 3,
    "samples_per_dataset": 2000, 
}

if os.environ.get("SMOKE_TEST"):
    print("ðŸ”¥ SMOKE TEST MODE ENABLED: Minimal dataset")
    CONFIG["runs_per_log"] = 1
    CONFIG["samples_per_dataset"] = 5

PROJECT_ROOT = Path(__file__).parent.parent.parent
LOGHUB_DIR = PROJECT_ROOT / "data" / "loghub"

LOGHUB_URLS = {
    "BGL": "https://raw.githubusercontent.com/logpai/loghub/master/BGL/BGL_2k.log",
    "HDFS": "https://raw.githubusercontent.com/logpai/loghub/master/HDFS/HDFS_2k.log",
}

# Fallback sample logs
BGL_SAMPLE = [
    {"raw": "- 1131511861 2005.11.09 R33-M0-N7-C:J03-U01 2005-11-09-06.11.01.134579 R33-M0-N7-C:J03-U01 RAS KERNEL INFO generating core.7681",
     "expected": {"timestamp": "2005-11-09-06.11.01", "level": "INFO", "component": "KERNEL", "message": "generating core.7681"}},
]

HDFS_SAMPLE = [
    {"raw": "081109 203615 148 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.19.102:54106",
     "expected": {"timestamp": "081109 203615", "level": "INFO", "component": "DataNode", "message": "Receiving block"}},
]


def download_loghub_dataset(dataset_name: str) -> bool:
    """Download LogHub dataset if not present."""
    dataset_dir = LOGHUB_DIR / dataset_name
    dataset_path = dataset_dir / f"{dataset_name}_2k.log"
    
    if dataset_path.exists():
        return True
    
    url = LOGHUB_URLS.get(dataset_name)
    if not url:
        return False
    
    try:
        dataset_dir.mkdir(parents=True, exist_ok=True)
        urllib.request.urlretrieve(url, dataset_path)
        return True
    except Exception as e:
        print(f"  âœ— Download failed: {e}")
        return False


def load_loghub_dataset(dataset_name: str, max_samples: int) -> list:
    """Load LogHub dataset."""
    dataset_path = LOGHUB_DIR / dataset_name / f"{dataset_name}_2k.log"
    
    if not dataset_path.exists():
        download_loghub_dataset(dataset_name)
    
    if not dataset_path.exists():
        return None
    
    logs = []
    with open(dataset_path, 'r', errors='ignore') as f:
        for i, line in enumerate(f):
            if i >= max_samples:
                break
            line = line.strip()
            if line:
                logs.append({"raw": line, "expected": None})
    
    print(f"  Loaded {len(logs)} logs from {dataset_path}")
    return logs


def run_parser_experiment() -> dict:
    """Test parser on LogHub datasets using actual LogParser."""
    print("=" * 70)
    print("EXPERIMENT 6: Parser Accuracy on LogHub (CORRECTED)")
    print("=" * 70)
    
    parser = LogParser(model=CONFIG["model"])
    
    fields = ["timestamp", "level", "component", "message"]
    results = {"datasets": {}}
    
    for ds_name, fallback in [("BGL", BGL_SAMPLE), ("HDFS", HDFS_SAMPLE)]:
        print(f"\nTesting {ds_name}...")
        
        logs = load_loghub_dataset(ds_name, CONFIG["samples_per_dataset"])
        if logs is None:
            print(f"  Using fallback sample data")
            logs = fallback
        
        field_correct = {f: 0 for f in fields}
        total_tests = 0
        latencies = []
        errors = 0
        
        for idx, log_entry in enumerate(logs):
            if idx % 100 == 0:
                print(f"  Progress: {idx}/{len(logs)}")
            
            for run in range(CONFIG["runs_per_log"]):
                start = time.time()
                try:
                    # Use actual parser
                    log_obj = parser.extract_log_info_by_llm(log_entry["raw"])
                    latency = time.time() - start
                    latencies.append(latency)
                    
                    # Convert to dict for field checking
                    parsed = log_obj.model_dump()
                    
                    # Check fields
                    for field in fields:
                        val = str(parsed.get(field, "")).strip()
                        if val and val.lower() not in ['none', 'null', 'n/a', '']:
                            field_correct[field] += 1
                            
                    total_tests += 1
                    
                except Exception:
                    errors += 1
                    latencies.append(time.time() - start)
                    total_tests += 1
        
        field_accuracy = {}
        for field in fields:
            acc = field_correct[field] / total_tests if total_tests > 0 else 0
            field_accuracy[field] = {"mean": round(acc * 100, 1), "std": 0.0}
        
        overall = sum(field_correct.values()) / (total_tests * len(fields)) if total_tests > 0 else 0
        
        # Ensure we report actual samples processed
        actual_samples = len(logs)
        
        results["datasets"][ds_name] = {
            "samples": actual_samples,
            "runs": CONFIG["runs_per_log"],
            "field_accuracy": field_accuracy,
            "overall_accuracy": round(overall * 100, 1),
            "latency": {
                "mean_s": round(statistics.mean(latencies), 3) if latencies else 0,
                "std_s": round(statistics.stdev(latencies), 3) if len(latencies) > 1 else 0,
            },
            "throughput_per_sec": round(1 / statistics.mean(latencies), 3) if latencies else 0,
            "total_errors": errors
        }
        
        print(f"  Overall: {overall*100:.1f}%")
    
    results["timestamp"] = datetime.now().isoformat()
    return results


def main():
    results_raw = run_parser_experiment()
    
    # Format output
    results = {
        "method": "GraphRCA LogParser (Actual)",
        "model": CONFIG["model"],
        "datasets": results_raw["datasets"]
    }
    
    # Add percentile stats
    for ds_name in results["datasets"]:
        ds = results["datasets"][ds_name]
        mean_s = ds["latency"]["mean_s"]
        std_s = ds["latency"]["std_s"]
        ds["latency"]["p50_s"] = round(mean_s, 3)
        ds["latency"]["p95_s"] = round(mean_s + 1.645 * std_s, 3)
    
    output_path = Path(__file__).parent / "data" / "02_llm_parsing.json"
    output_path.parent.mkdir(exist_ok=True)
    with open(output_path, "w") as f:
        json.dump(results, f, indent=2)
    
    print(f"\nâœ“ Results saved to {output_path}")


if __name__ == "__main__":
    main()
